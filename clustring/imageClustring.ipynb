{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Part 0: Featurs Extraction</h1>\n",
    "<h2>Why we extract features and dont just read the pixels?</h2>\n",
    "<p>In image clustering, extracting features from images is a crucial step for several reasons. Reading the pixels directly may not be efficient or effective in capturing the essential information about the images, which is necessary for clustering.here are some important reason for this:</p>\n",
    "<ul>\n",
    "    <li><b>Dimensionality Reduction:</b>Images contain a vast amount of data, as they have a high number of pixels. Reading the pixels directly would involve dealing with a large number of dimensions. Feature extraction helps reduce this dimensionality, making the process more manageable and computationally efficient.</li>\n",
    "    <li><b>Robustness to Noise:</b>Images can contain noise or variations in lighting, which may affect the raw pixel values. Feature extraction helps to filter out noise and focus on the essential characteristics of the images, making the clustering process more robust and accurate.</li>\n",
    "    <li><b>Generalization:</b>Feature extraction allows the algorithm to generalize and recognize patterns across different images. This generalization helps in clustering images that may not be identical but share similar features or characteristics.</li>\n",
    "</ul>\n",
    "<h2>Techniques</h2>\n",
    "<p>\n",
    "There are various techniques for feature extraction in image processing. Some of the most common methods include:</p>\n",
    "<ol>\n",
    "    <li><b>Principal Component Analysis (PCA):</b> PCA is a linear technique that transforms the original image data into a new coordinate system, where the axes represent the principal components of the image data. It helps reduce dimensionality while retaining most of the image's information.</li>\n",
    "    <li><b>Singular Value Decomposition (SVD):</b>SVD is another linear transformation technique similar to PCA. It decomposes the image matrix into three matrices (U, Σ, and V). The columns of U and V represent the features, and the diagonal elements of Σ represent the importance of each feature. SVD is often used for dimensionality reduction in image processing.</li>\n",
    "    <li><b>Convolutional Neural Networks (CNNs):</b>CNNs are a deep learning-based approach to feature extraction. They learn hierarchical feature representations by applying multiple layers of convolutions and pooling operations. CNNs can automatically extract relevant features from raw image data, making them powerful tools for various computer vision tasks, such as image classification, object detection, and segmentation.</li>\n",
    "</ol>\n",
    "<h2>what preprocessing need before images entered on model?</h2>\n",
    "<p>Before entering images into a model, it's essential to preprocess them to ensure they are suitable for the machine learning or deep learning algorithms. Here are some important preprocessing steps for image data:</p>\n",
    "<ul>\n",
    "    <li><b>Resizing:</b>Resize the images to a uniform size that matches the input requirements of your model. This helps the model process images more efficiently and reduces computational complexity.</li>\n",
    "    <li><b>Normalization:</b>Normalize the pixel values of the images to a specific range, typically between 0 and 1 or -1 and 1. This step ensures that all images have similar pixel values, preventing any one image from dominating the learning process due to its high intensity or contrast.</li>\n",
    "    <li><b>Label Encoding (for supervised learning):</b>If your images have associated labels or categories, encode these labels as numerical values. This is necessary for many supervised learning algorithms that require numerical input.</li>\n",
    "</ul>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7d04e7929acd3ef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Part1: Kmeans vs. DBSCAN</h1>\n",
    "<p>K-Means and DBSCAN are both popular clustering algorithms used in machine learning and data analysis for partitioning data into distinct groups based on their similarities. Here's an overview of each method along with their advantages and disadvantages:\n",
    "</p>\n",
    "<h2>K-Means</h2>\n",
    "<ol>\n",
    "    <li><b>Algorithm Overview:</b> K-Means is a centroid-based clustering algorithm where the data points are assigned to the nearest centroid. The algorithm iteratively updates the centroids until convergence, minimizing the within-cluster variance.</li>\n",
    "   <li>\n",
    "   <b>Advantages:</b>\n",
    "    <ul>\n",
    "        <li><b>Simplicity:</b> K-Means is straightforward to understand and implement.</li>\n",
    "        <li><b>Scalability:</b>It works well with large datasets, as its time complexity is linear with the number of data points.</li>\n",
    "        <li><b>Efficiency:</b>K-Means converges relatively quickly, especially with well-separated clusters.</li>\n",
    "    </ul>\n",
    "   </li>\n",
    "   <li>\n",
    "   <b>Disadvantages:</b>\n",
    "    <ul>\n",
    "        <li><b>Sensitive to Initial Centroid Selection: </b>Different initializations can lead to different final clusters, impacting the algorithm's performance.</li>\n",
    "        <li><b>Assumes Spherical Clusters:</b>K-Means assumes that clusters are spherical and of similar size, which may not hold true for all datasets.</li>\n",
    "        <li><b>Need to Specify the Number of Clusters (K):</b>Determining the optimal number of clusters (K) can be challenging and may require domain knowledge or heuristics.</li>\n",
    "    </ul>\n",
    "    </li>\n",
    "</ol>\n",
    "<h2>DBSCAN</h2>\n",
    "<ol>\n",
    "    <li><b>Algorithm Overview:</b>DBSCAN is a density-based clustering algorithm that groups together data points that are closely packed, marking outliers as noise. It does not require the number of clusters to be specified in advance.</li>\n",
    "   <li>\n",
    "   <b>Advantages:</b>\n",
    "    <ul>\n",
    "        <li><b>Ability to Detect Arbitrary Shaped Clusters:</b>DBSCAN can identify clusters of various shapes and sizes, unlike K-Means, which assumes spherical clusters.</li>\n",
    "        <li><b>Robust to Noise:</b>DBSCAN automatically identifies and ignores outliers as noise, making it robust to noise in the dataset.</li>\n",
    "        <li><b>No Need to Specify the Number of Clusters:</b>DBSCAN determines the number of clusters automatically based on the data density.</li>\n",
    "    </ul>\n",
    "   </li>\n",
    "   <li>\n",
    "   <b>Disadvantages:</b>\n",
    "    <ul>\n",
    "        <li><b>Sensitivity to Distance Metric and Epsilon Parameter:</b>The performance of DBSCAN can be influenced by the choice of distance metric and the epsilon parameter, which defines the neighborhood size.</li>\n",
    "        <li><b>Difficulty in Handling Varying Density: </b>DBSCAN may struggle with datasets containing clusters of varying densities, as it relies on a single epsilon parameter for density estimation.</li>\n",
    "        <li><b>Computationally Intensive for Large Datasets: </b>DBSCAN's time complexity is higher compared to K-Means, especially for large datasets, as it needs to calculate distances between each pair of data points.</li>\n",
    "    </ul>\n",
    "    </li>\n",
    "</ol>\n",
    "<h2>Comparison:</h2>\n",
    "<ul>\n",
    "    <li><b>Flexibility:</b>DBSCAN is more flexible in identifying clusters of arbitrary shapes and sizes, while K-Means assumes spherical clusters.</li>\n",
    "    <li><b>Noise Handling:</b>DBSCAN automatically identifies and ignores outliers, whereas K-Means may assign noise points to the nearest centroid.</li>\n",
    "    <li><b>Parameter Sensitivity:</b>DBSCAN requires tuning of parameters like epsilon and minPts, while K-Means primarily requires specifying the number of clusters (K).</li>\n",
    "    <li><b>Scalability:</b>K-Means tends to be more scalable and efficient for large datasets compared to DBSCAN.</li>\n",
    "</ul>\n",
    "<p>Choosing between K-Means and DBSCAN depends on the dataset characteristics, the desired cluster shapes, and the presence of noise. For datasets with well-defined, spherical clusters and known or easily determinable K, K-Means may be preferable. On the other hand, for datasets with complex shapes, varying densities, or significant noise, DBSCAN might yield better results.</p>\n",
    "<h2>PCA</h2>\n",
    "<p>PCA stands for Principal Component Analysis. It's a statistical technique used for dimensionality reduction in data analysis and machine learning. The primary goal of PCA is to reduce the dimensionality of a dataset while retaining as much of the variation present in the original dataset as possible. This reduction in dimensionality helps in simplifying the dataset, making it easier to visualize, analyze, and process, while still capturing the essential features of the data.</p>\n",
    "<b>how does it work?</b>\n",
    "<ol>\n",
    "    <li><b>Data Standardization:</b>PCA typically starts with standardizing the data to have a mean of 0 and a standard deviation of 1 across each feature</li>\n",
    "    <li><b>Covariance Matrix Computation:</b> PCA computes the covariance matrix of the standardized data</li>\n",
    "    <li><b>Eigenvalue Decomposition:</b>The covariance matrix is then decomposed into its eigenvectors and eigenvalues.</li>\n",
    "    <li><b>Selection of Principal Components:</b>The eigenvectors are ranked in order of their corresponding eigenvalues</li>\n",
    "    <li><b>Projection:</b> Finally, the original data is projected onto the subspace spanned by the selected principal components.</li>\n",
    "</ol>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d93a0d41bd291e17"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-22T16:55:48.972440Z",
     "start_time": "2024-04-22T16:55:32.209735Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m7/7\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m15s\u001B[0m 2s/step\n",
      "[ 0  0  1  0  0  0  0  0  0  0  0  0  0 -1  2  0  0  0  0  0  3  4  2  4\n",
      "  0  0  1  0 -1  0]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from keras.applications import VGG16\n",
    "from keras.models import Model\n",
    "from sklearn.cluster import KMeans , DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "image_folder = \"flower_images\"\n",
    "csv_file = \"flower_labels.csv\"\n",
    "true_labels = pd.read_csv(csv_file)['label'].values\n",
    "\n",
    "image_files = os.listdir(image_folder)\n",
    "image_size = (224,224)\n",
    "\n",
    "def image_preprocess(path,target_size):\n",
    "    image = cv2.imread(path)\n",
    "    image = cv2.resize(image,target_size)\n",
    "    image = image / 255.0\n",
    "    return image\n",
    "\n",
    "images = []\n",
    "\n",
    "for image_file in image_files :\n",
    "    image_path = os.path.join(image_folder,image_file)\n",
    "    image = image_preprocess(image_path,image_size)\n",
    "    images.append(image)\n",
    "\n",
    "images = np.array(images)\n",
    "\n",
    "base_model = VGG16(weights='imagenet',include_top=\"False\",input_shape=(image_size[0],image_size[1],3))\n",
    "conv_output = base_model.get_layer('block5_conv3').output\n",
    "features_extractor = Model(inputs=base_model.input,outputs=conv_output)\n",
    "features = features_extractor.predict(images)\n",
    "features = features.reshape(features.shape[0],-1)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "reduced_features = pca.fit_transform(features)\n",
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters=10)\n",
    "kmeans_clusters = kmeans.fit_predict(reduced_features)\n",
    "\n",
    "dbscan = DBSCAN(eps=3.2,min_samples=3)\n",
    "dbscan_clusters = dbscan.fit_predict(reduced_features)\n",
    "\n",
    "cluster_index = np.where(dbscan_clusters == dbscan_clusters[0])[0][0]\n",
    "cluster_index_c = np.where(kmeans_clusters == kmeans_clusters[0])[0][0]\n",
    "\n",
    "\n",
    "# print(kmeans_clusters[:30])\n",
    "# print(true_labels[:30])\n",
    "print(dbscan_clusters[:30])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>the appropriate K is approximatley same as true labels number that we can set it with test and fail and evaluate our final labaling with real labels </h1>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5c286027b750f34"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Validating</h1>\n",
    "<p>Homogeneity and silhouette are both validation metrics commonly used to assess the quality of clustering algorithms, including those applied to image clustering.</p>\n",
    "<ol>\n",
    "    <li><b>Homogeneity: </b>Homogeneity measures the degree to which each cluster contains only members of a single class. In the context of image clustering, homogeneity assesses whether the clusters formed by the algorithm represent distinct and homogeneous groups of similar images. A high homogeneity score indicates that the clusters are composed mostly of images from the same class or category, while a low score suggests that the clusters contain mixed or heterogeneous images.</li>\n",
    "    <li><b>Silhouette Score: </b> Silhouette analysis measures how similar an object is to its own cluster (cohesion) compared to other clusters (separation). For each sample, the silhouette score ranges from -1 to 1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. A silhouette score close to 1 suggests that the sample is appropriately clustered, while a score near -1 indicates that the sample might be misclassified. In the context of image clustering, silhouette analysis helps to evaluate the compactness and separation of clusters formed by the algorithm.</li>\n",
    "</ol>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb1132d0810d673f"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity score for KMeans: 0.3769877861494049\n",
      "Silhouette score for KMeans: 0.35088867\n",
      "Homogeneity score for DBSCAN: 0.20004280943208474\n",
      "Silhouette score for DBSCAN: 0.11665531\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import homogeneity_score , silhouette_score\n",
    "homogeneity_kmeans = homogeneity_score(true_labels, kmeans_clusters)\n",
    "print(\"Homogeneity score for KMeans:\", homogeneity_kmeans)\n",
    "\n",
    "silhouette_kmeans = silhouette_score(reduced_features, kmeans_clusters)\n",
    "print(\"Silhouette score for KMeans:\", silhouette_kmeans)\n",
    "\n",
    "homogeneity_dbscan = homogeneity_score(true_labels, dbscan_clusters)\n",
    "print(\"Homogeneity score for DBSCAN:\", homogeneity_dbscan)\n",
    "\n",
    "silhouette_dbscan = silhouette_score(reduced_features, dbscan_clusters)\n",
    "print(\"Silhouette score for DBSCAN:\", silhouette_dbscan)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T16:56:01.293648Z",
     "start_time": "2024-04-22T16:56:01.265320Z"
    }
   },
   "id": "369dbd6d9c3e6476",
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
