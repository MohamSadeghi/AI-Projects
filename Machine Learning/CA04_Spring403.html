<html>
<head>
<title>CA04_Spring403.ipynb</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #7a7e85;}
.s1 { color: #cf8e6d;}
.s2 { color: #bcbec4;}
.s3 { color: #bcbec4;}
.s4 { color: #6aab73;}
.s5 { color: #2aacb8;}
</style>
</head>
<body bgcolor="#1e1f22">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
CA04_Spring403.ipynb</font>
</center></td></tr></table>
<pre><span class="s0">#%% 
</span><span class="s1">import </span><span class="s2">numpy </span><span class="s1">as </span><span class="s2">np</span>
<span class="s1">import </span><span class="s2">pandas </span><span class="s1">as </span><span class="s2">pd</span>
<span class="s1">import </span><span class="s2">seaborn </span><span class="s1">as </span><span class="s2">sb</span>
<span class="s1">import </span><span class="s2">matplotlib</span><span class="s3">.</span><span class="s2">pyplot </span><span class="s1">as </span><span class="s2">plt</span>
<span class="s1">from </span><span class="s2">sklearn</span><span class="s3">.</span><span class="s2">impute </span><span class="s1">import </span><span class="s2">KNNImputer</span>
<span class="s1">from </span><span class="s2">sklearn</span><span class="s3">.</span><span class="s2">preprocessing </span><span class="s1">import </span><span class="s2">StandardScaler</span>
<span class="s1">from </span><span class="s2">sklearn</span><span class="s3">.</span><span class="s2">model_selection </span><span class="s1">import </span><span class="s2">train_test_split</span><span class="s3">,</span><span class="s2">GridSearchCV</span><span class="s3">,</span><span class="s2">RandomizedSearchCV</span>
<span class="s1">from </span><span class="s2">sklearn</span><span class="s3">.</span><span class="s2">metrics </span><span class="s1">import </span><span class="s2">accuracy_score</span><span class="s3">,</span><span class="s2">confusion_matrix</span><span class="s3">,</span><span class="s2">classification_report</span>
<span class="s1">from </span><span class="s2">sklearn</span><span class="s3">.</span><span class="s2">tree </span><span class="s1">import </span><span class="s2">DecisionTreeClassifier</span><span class="s3">,</span><span class="s2">plot_tree</span>
<span class="s1">from </span><span class="s2">sklearn</span><span class="s3">.</span><span class="s2">neighbors </span><span class="s1">import </span><span class="s2">KNeighborsClassifier</span>
<span class="s1">from </span><span class="s2">sklearn</span><span class="s3">.</span><span class="s2">ensemble </span><span class="s1">import  </span><span class="s2">RandomForestClassifier</span>
<span class="s1">from </span><span class="s2">sklearn</span><span class="s3">.</span><span class="s2">svm </span><span class="s1">import </span><span class="s2">SVC</span>
<span class="s0">#%% md 
</span><span class="s2">&lt;h1&gt;Part 1: Dataset Analysis&lt;/h1&gt; 
</span><span class="s0">#%% 
</span><span class="s2">dataPath </span><span class="s3">= </span><span class="s4">&quot;DataSet.csv&quot;</span>
<span class="s2">dataset </span><span class="s3">= </span><span class="s2">pd</span><span class="s3">.</span><span class="s2">read_csv</span><span class="s3">(</span><span class="s2">dataPath</span><span class="s3">)</span>
<span class="s2">display</span><span class="s3">(</span><span class="s2">dataset</span><span class="s3">)</span>
<span class="s0">#%% md 
</span><span class="s2">&lt;h2&gt;1-describe and info&lt;/h2&gt; 
</span><span class="s0">#%% 
</span><span class="s2">dataset</span><span class="s3">.</span><span class="s2">info</span><span class="s3">()</span>
<span class="s0">#%% md 
</span><span class="s2">&lt;p&gt;As the info() method shows, the dataset has 14 columns and 506 rows. The columns are described as follows:&lt;/p&gt; 
&lt;ul style=&quot;display: flex; flex-direction: column; gap: 10px;&quot;&gt; 
 &lt;li&gt;CRIM: &lt;div style=&quot;padding:0px 5px 0px 5px;background-color:gray;color:black&quot;&gt;type:float64&lt;/div&gt;&lt;/li&gt; 
 &lt;li&gt;ZN: &lt;div style=&quot;padding:0px 5px 0px 5px;background-color:gray;color:black&quot;&gt;type:integer(float64 in dataset because of missing values)&lt;/div&gt;&lt;/li&gt; 
 &lt;li&gt;INDUS: &lt;div style=&quot;padding:0px 5px 0px 5px;background-color:gray;color:black&quot;&gt;type:float64&lt;/div&gt;&lt;/li&gt; 
 &lt;li&gt;CHAS: &lt;div style=&quot;padding:0px 5px 0px 5px;background-color:gray;color:black&quot;&gt;type:integer(float64 in dataset because of missing values)&lt;/div&gt;&lt;/li&gt; 
 &lt;li&gt;NOX: &lt;div style=&quot;padding:0px 5px 0px 5px;background-color:gray;color:black&quot;&gt;type:float64&lt;/div&gt;&lt;/li&gt; 
 &lt;li&gt;RM: &lt;div style=&quot;padding:0px 5px 0px 5px;background-color:gray;color:black&quot;&gt;type:float64&lt;/div&gt;&lt;/li&gt; 
 &lt;li&gt;AGE: &lt;div style=&quot;padding:0px 5px 0px 5px;background-color:gray;color:black&quot;&gt;type:float64&lt;/div&gt;&lt;/li&gt; 
 &lt;li&gt;DIS: &lt;div style=&quot;padding:0px 5px 0px 5px;background-color:gray;color:black&quot;&gt;type:float64&lt;/div&gt;&lt;/li&gt; 
 &lt;li&gt;RAD: &lt;div style=&quot;padding:0px 5px 0px 5px;background-color:gray;color:black&quot;&gt;type:integer&lt;/div&gt;&lt;/li&gt; 
 &lt;li&gt;TAX: &lt;div style=&quot;padding:0px 5px 0px 5px;background-color:gray;color:black&quot;&gt;type:float64&lt;/div&gt;&lt;/li&gt; 
 &lt;li&gt;PRTATIO: &lt;div style=&quot;padding:0px 5px 0px 5px;background-color:gray;color:black&quot;&gt;type:float64&lt;/div&gt;&lt;/li&gt; 
 &lt;li&gt;B: &lt;div style=&quot;padding:0px 5px 0px 5px;background-color:gray;color:black&quot;&gt;type:float64&lt;/div&gt;&lt;/li&gt; 
 &lt;li&gt;LSTAT: &lt;div style=&quot;padding:0px 5px 0px 5px;background-color:gray;color:black&quot;&gt;type:float64&lt;/div&gt;&lt;/li&gt; 
&lt;/ul&gt; 
</span><span class="s0">#%% 
</span><span class="s2">dataset</span><span class="s3">.</span><span class="s2">describe</span><span class="s3">()</span>
<span class="s0">#%% md 
</span><span class="s2">&lt;p&gt;describe method show bellow informaton for each featur:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;count:number of non-missing values&lt;/li&gt; 
 &lt;li&gt;mean:Mean of the values&lt;/li&gt; 
 &lt;li&gt;std:Standard deviation of the values&lt;/li&gt; 
 &lt;li&gt;min:Minimum of values&lt;/li&gt; 
 &lt;li&gt;25%:First quartile&lt;/li&gt; 
 &lt;li&gt;50%:median of values&lt;/li&gt; 
 &lt;li&gt;75%:Third quartile&lt;/li&gt; 
 &lt;li&gt;max:Maximum of value&lt;/li&gt; 
&lt;/ul&gt; 
</span><span class="s0">#%% md 
</span><span class="s2">&lt;h2&gt;2-number of missing values and their ratio in dataset&lt;/h2&gt; 
</span><span class="s0">#%% 
</span><span class="s2">rowNumber </span><span class="s3">= </span><span class="s2">dataset</span><span class="s3">.</span><span class="s2">shape</span><span class="s3">[</span><span class="s5">0</span><span class="s3">]</span>
<span class="s1">for </span><span class="s2">column </span><span class="s1">in </span><span class="s2">dataset</span><span class="s3">.</span><span class="s2">columns</span><span class="s3">:</span>
    <span class="s2">Nan </span><span class="s3">= </span><span class="s2">dataset</span><span class="s3">[</span><span class="s2">column</span><span class="s3">].</span><span class="s2">isna</span><span class="s3">().</span><span class="s2">sum</span><span class="s3">()</span>
    <span class="s2">print</span><span class="s3">(</span><span class="s4">f&quot;number of NaN values in column </span><span class="s1">{</span><span class="s2">column</span><span class="s1">:</span><span class="s4">5</span><span class="s1">}</span><span class="s4">-&gt; </span><span class="s1">{</span><span class="s2">Nan</span><span class="s1">:</span><span class="s4">3</span><span class="s1">} </span><span class="s4">(</span><span class="s1">{</span><span class="s2">Nan</span><span class="s3">/</span><span class="s2">rowNumber</span><span class="s3">*</span><span class="s5">100</span><span class="s1">:</span><span class="s4">5.2f</span><span class="s1">}</span><span class="s4">%)&quot;</span><span class="s3">)</span>
<span class="s0">#%% md 
</span><span class="s2">&lt;h2&gt;3-plotting Unique Value for features&lt;/h2&gt; 
&lt;p&gt;this code generates a series of count plots, one for each feature (column) in the dataset. Each plot shows the distribution of unique values for that feature. This is useful for understanding the frequency distribution of categorical variables in the dataset.&lt;/p&gt; 
</span><span class="s0">#%% 
</span><span class="s1">for </span><span class="s2">column </span><span class="s1">in </span><span class="s2">dataset</span><span class="s3">.</span><span class="s2">columns</span><span class="s3">:</span>
    <span class="s2">plt</span><span class="s3">.</span><span class="s2">figure</span><span class="s3">(</span><span class="s2">figsize</span><span class="s3">=(</span><span class="s5">15</span><span class="s3">,</span><span class="s5">5</span><span class="s3">))</span>
    <span class="s2">plt</span><span class="s3">.</span><span class="s2">xticks</span><span class="s3">(</span><span class="s2">rotation</span><span class="s3">=</span><span class="s5">90</span><span class="s3">)</span>
    <span class="s2">sb</span><span class="s3">.</span><span class="s2">countplot</span><span class="s3">(</span><span class="s2">x</span><span class="s3">=</span><span class="s2">column</span><span class="s3">,</span><span class="s2">data</span><span class="s3">=</span><span class="s2">dataset</span><span class="s3">)</span>
<span class="s0">#%% 
</span><span class="s2">dataset</span><span class="s3">.</span><span class="s2">hist</span><span class="s3">(</span><span class="s2">figsize</span><span class="s3">=(</span><span class="s5">15</span><span class="s3">,</span><span class="s5">15</span><span class="s3">))</span>
<span class="s2">plt</span><span class="s3">.</span><span class="s2">show</span><span class="s3">()</span>
<span class="s0">#%% md 
</span><span class="s2">&lt;h2&gt;4-relation between features&lt;/h2&gt; 
&lt;p&gt;The heatmap visually represents the correlation between different pairs of features. Each cell in the heatmap represents the correlation coefficient between two features, with colors indicating the strength and direction of the correlation. Positive correlations are typically represented by lighter colors, while negative correlations are represented by darker colors. A correlation coefficient close to 1 or -1 indicates a strong correlation, while a coefficient close to 0 indicates little to no correlation.&lt;/p&gt; 
</span><span class="s0">#%% 
</span><span class="s2">plt</span><span class="s3">.</span><span class="s2">figure</span><span class="s3">(</span><span class="s2">figsize</span><span class="s3">=(</span><span class="s5">15</span><span class="s3">,</span><span class="s5">15</span><span class="s3">))</span>
<span class="s2">sb</span><span class="s3">.</span><span class="s2">heatmap</span><span class="s3">(</span><span class="s2">dataset</span><span class="s3">.</span><span class="s2">corr</span><span class="s3">(),</span><span class="s2">annot</span><span class="s3">=</span><span class="s1">True</span><span class="s3">,</span><span class="s2">fmt</span><span class="s3">=</span><span class="s4">'.0%'</span><span class="s3">)</span>
<span class="s0">#%% md 
</span><span class="s2">&lt;h2&gt;5-plotting scatter and hexbin&lt;/h2&gt; 
&lt;ol&gt; 
&lt;li&gt;&lt;b&gt;Scatter plot: &lt;/b&gt;&lt;ul&gt; 
 &lt;li&gt;A scatter plot is a type of plot that displays individual data points as dots on a two-dimensional plane, with one variable on the x-axis and another variable on the y-axis.&lt;/li&gt; 
 &lt;li&gt;Each dot represents a single observation in the dataset, and the position of the dot corresponds to the values of the two variables it represents.&lt;/li&gt; 
 &lt;li&gt;Scatter plots are commonly used to visually assess the relationship or correlation between two variables. They are particularly useful for identifying patterns, trends, clusters, or outliers in the data.&lt;/li&gt; 
&lt;/ul&gt;&lt;/li&gt; 
&lt;li&gt;&lt;b&gt;Hexbin plot: &lt;/b&gt;&lt;ul&gt; 
&lt;li&gt;A hexbin plot is a type of two-dimensional histogram where the data space is divided into hexagonal bins, and the number of observations falling into each bin is represented by a color intensity or density.&lt;/li&gt; 
 &lt;li&gt;Similar to a scatter plot, a hexbin plot displays the relationship between two continuous variables. However, instead of plotting individual data points, hexbin plots aggregate the data into hexagonal bins and represent the density of observations within each bin.&lt;/li&gt; 
 &lt;li&gt;Hexbin plots are particularly useful for visualizing the distribution of large datasets, especially when there is overplotting in scatter plots, which occurs when multiple data points overlap.&lt;/li&gt; 
&lt;/ul&gt;&lt;/li&gt; 
&lt;/ol&gt; 
</span><span class="s0">#%% 
</span><span class="s1">for </span><span class="s2">column </span><span class="s1">in </span><span class="s2">dataset</span><span class="s3">.</span><span class="s2">columns</span><span class="s3">:</span>
    <span class="s1">if </span><span class="s2">column </span><span class="s3">== </span><span class="s4">'MEDV'</span><span class="s3">:</span>
        <span class="s1">continue</span>
    <span class="s2">sb</span><span class="s3">.</span><span class="s2">relplot</span><span class="s3">(</span><span class="s2">x</span><span class="s3">=</span><span class="s2">column</span><span class="s3">, </span><span class="s2">y</span><span class="s3">=</span><span class="s4">'MEDV'</span><span class="s3">, </span><span class="s2">data</span><span class="s3">=</span><span class="s2">dataset</span><span class="s3">, </span><span class="s2">kind</span><span class="s3">=</span><span class="s4">'scatter'</span><span class="s3">)</span>
<span class="s0">#%% 
</span><span class="s1">for </span><span class="s2">column </span><span class="s1">in </span><span class="s2">dataset</span><span class="s3">.</span><span class="s2">columns</span><span class="s3">:</span>
    <span class="s1">if </span><span class="s2">column </span><span class="s3">== </span><span class="s4">'MEDV'</span><span class="s3">:</span>
        <span class="s1">continue</span>
    <span class="s2">sb</span><span class="s3">.</span><span class="s2">jointplot</span><span class="s3">(</span><span class="s2">x</span><span class="s3">=</span><span class="s2">column</span><span class="s3">, </span><span class="s2">y</span><span class="s3">=</span><span class="s4">'MEDV'</span><span class="s3">, </span><span class="s2">data</span><span class="s3">=</span><span class="s2">dataset</span><span class="s3">, </span><span class="s2">kind</span><span class="s3">=</span><span class="s4">'hex'</span><span class="s3">)</span>
<span class="s0">#%% md 
</span><span class="s2">&lt;hr/&gt; 
&lt;h1&gt;Part2:Preprocessing dataset&lt;/h1&gt; 
&lt;h2&gt;7-Filling missing values&lt;/h2&gt; 
&lt;ol&gt; 
    &lt;li&gt;&lt;b&gt;Mean/Median/Mode Imputation: &lt;/b&gt;Replace missing values with the mean,std or mode of the column.These methods are simple and quick. They preserve the mean, median, or mode of the data, which can be important for some algorithms. However, they can introduce bias if the data is not normally distributed.&lt;/li&gt; 
    &lt;li&gt;&lt;b&gt;Forward Fill/Backward Fill: &lt;/b&gt; Replace missing values with the last or next observed value.Useful for time series data where missing values are likely to be continuous. However, they may not be suitable for all types of data and can propagate errors if the order of data is important.&lt;/li&gt; 
    &lt;li&gt;&lt;b&gt;Interpolation: &lt;/b&gt;Estimate missing values based on the linear or higher-order polynomial relationship between adjacent data points.this methods can capture trends in the data more accurately, especially when the data has a clear pattern or trend. However, they may not perform well if the data is highly irregular or noisy.&lt;/li&gt; 
    &lt;li&gt;&lt;b&gt;K-Nearest Neighbors (KNN) Imputation: &lt;/b&gt;Replace missing values with the mean/median/mode value of the K nearest neighbors of each sample with missing features.KNN imputation considers the similarity between data points, making it suitable for datasets where observations with similar features tend to have similar values.&lt;/li&gt; 
    &lt;li&gt;&lt;b&gt;Random Imputation: &lt;/b&gt;Replace missing values with randomly selected values from the distribution of non-missing values in the dataset.Random imputation can help preserve the variability of the data and avoid introducing biases that may occur with deterministic imputation methods. &lt;/li&gt; 
&lt;/ol&gt; 
</span><span class="s0">#%% 
</span><span class="s2">mean_filled </span><span class="s3">= </span><span class="s2">dataset</span><span class="s3">.</span><span class="s2">fillna</span><span class="s3">(</span><span class="s2">dataset</span><span class="s3">.</span><span class="s2">mean</span><span class="s3">())</span>
<span class="s2">forward_filled </span><span class="s3">= </span><span class="s2">dataset</span><span class="s3">.</span><span class="s2">ffill</span><span class="s3">()</span>
<span class="s2">forward_filled</span><span class="s3">.</span><span class="s2">describe</span><span class="s3">()</span>
<span class="s0">#%% md 
</span><span class="s2">&lt;h2&gt;KNN&lt;/h2&gt; 
</span><span class="s0">#%% 
</span><span class="s2">imputer </span><span class="s3">= </span><span class="s2">KNNImputer</span><span class="s3">(</span><span class="s2">n_neighbors</span><span class="s3">=</span><span class="s5">4</span><span class="s3">)</span>
<span class="s2">imputed </span><span class="s3">= </span><span class="s2">imputer</span><span class="s3">.</span><span class="s2">fit_transform</span><span class="s3">(</span><span class="s2">dataset</span><span class="s3">)</span>
<span class="s2">imputed </span><span class="s3">= </span><span class="s2">pd</span><span class="s3">.</span><span class="s2">DataFrame</span><span class="s3">(</span><span class="s2">imputed</span><span class="s3">,</span><span class="s2">columns</span><span class="s3">=</span><span class="s2">dataset</span><span class="s3">.</span><span class="s2">columns</span><span class="s3">)</span>
<span class="s2">imputed</span><span class="s3">.</span><span class="s2">describe</span><span class="s3">()</span>
<span class="s0">#%% md 
</span><span class="s2">&lt;h2&gt;8-Removing columns&lt;/h2&gt; 
&lt;p&gt;as CHAS and DIS columns has correlation less than 20% with target according to heatmap,we can remove them&lt;/p&gt; 
</span><span class="s0">#%% 
</span><span class="s2">imputed</span><span class="s3">.</span><span class="s2">drop</span><span class="s3">([</span><span class="s4">'DIS'</span><span class="s3">, </span><span class="s4">'CHAS'</span><span class="s3">], </span><span class="s2">axis</span><span class="s3">=</span><span class="s5">1</span><span class="s3">, </span><span class="s2">inplace</span><span class="s3">=</span><span class="s1">True</span><span class="s3">)</span>
<span class="s2">imputed</span><span class="s3">.</span><span class="s2">describe</span><span class="s3">()</span>
<span class="s0">#%% md 
</span><span class="s2">&lt;h2&gt;9-Numerical and Categorical Features&lt;/h2&gt; 
&lt;p&gt;&lt;span style=&quot;color:green&quot;&gt;Numerical features&lt;/span&gt; are those that represent numerical values, which can be continuous or discrete.&lt;span style=&quot;color:green&quot;&gt;Categorical features&lt;/span&gt;, on the other hand, represent values from a discrete set or category. These values are limited to a specific number of groups or categories. Examples of categorical features include: gender (male, female), color (red, blue, green), neighborhood (north, south, east, west), etc.&lt;/p&gt; 
&lt;p&gt;The main difference between numerical and categorical features lies in the type of information they provide. Numerical features usually provide information about the magnitude or quantity of something , whereas categorical features typically provide information about the type or category of something &lt;/p&gt; 
&lt;b&gt;in our dataset: 
&lt;ul&gt; 
   &lt;li&gt;Numeric: CIRM - ZN - INDUS - NOX - RM - AGE - DIS - TAX - PTRATIO - B - LSTAT - MEDV&lt;/li&gt; 
   &lt;li&gt;Categoric: CHAS - RAD - &lt;/li&gt; 
&lt;/ul&gt; 
&lt;/b&gt; 
</span><span class="s0">#%% md 
</span><span class="s2">&lt;h2&gt;10-Normalization and Standardizing&lt;/h2&gt; 
&lt;p&gt;&lt;span style=&quot;color:yellow&quot;&gt;Normalization &lt;/span&gt;scales the numerical features to a range between 0 and 1. It is particularly useful when the features have varying scales.&lt;span style=&quot;color:yellow&quot;&gt;Standardization &lt;/span&gt;scales the numerical features to have a mean of 0 and a standard deviation of 1. It transforms the features so that they have a standard normal distribution, which is centered around zero with a unit standard deviation.&lt;/p&gt; 
&lt;p&gt;we can not use normalization because this methos is useful for algorithms which are based on distance like KNN or Neural Network.&lt;/p&gt; 
&lt;p&gt;we just standardize dataset to set each feature mean 0 and std to 1&lt;/p&gt; 
</span><span class="s0">#%% 
</span><span class="s2">standardize </span><span class="s3">= </span><span class="s2">StandardScaler</span><span class="s3">().</span><span class="s2">fit_transform</span><span class="s3">(</span><span class="s2">imputed</span><span class="s3">.</span><span class="s2">drop</span><span class="s3">(</span><span class="s4">'MEDV'</span><span class="s3">,</span><span class="s2">axis</span><span class="s3">=</span><span class="s5">1</span><span class="s3">))</span>
<span class="s2">standardize </span><span class="s3">= </span><span class="s2">pd</span><span class="s3">.</span><span class="s2">DataFrame</span><span class="s3">(</span><span class="s2">standardize</span><span class="s3">,</span><span class="s2">columns</span><span class="s3">=</span><span class="s2">imputed</span><span class="s3">.</span><span class="s2">columns</span><span class="s3">[:-</span><span class="s5">1</span><span class="s3">])</span>
<span class="s2">standardize</span><span class="s3">[</span><span class="s4">'MEDV'</span><span class="s3">] = </span><span class="s2">imputed</span><span class="s3">[</span><span class="s4">'MEDV'</span><span class="s3">]</span>
<span class="s2">standardize</span><span class="s3">.</span><span class="s2">describe</span><span class="s3">()</span>
<span class="s0">#%% md 
</span><span class="s2">&lt;h2&gt;11-Handling Categorical features&lt;/h2&gt; 
&lt;p&gt;For categorical features represented as strings or objects, several preprocessing techniques can be useful before training a machine learning model.&lt;/p&gt; 
&lt;ol&gt; 
    &lt;li&gt;&lt;b&gt;One-Hot Encoding: &lt;/b&gt;This technique converts categorical variables into a binary format, where each category becomes a separate binary feature. For each categorical feature, if there are n unique categories, one-hot encoding creates n binary features, each representing one category. &lt;/li&gt; 
    &lt;li&gt;&lt;b&gt;Label Encoding: &lt;/b&gt; Label encoding assigns a unique integer to each category. This technique is useful when there is ordinality in the categories,when the categories have a natural order.&lt;/li&gt; 
    &lt;li&gt;&lt;b&gt;Frequency Encoding: &lt;/b&gt;Frequency encoding replaces each category with the frequency of that category in the dataset. This can be useful when the frequency of occurrence of categories is informative and may contribute to predicting the target variable.&lt;/li&gt; 
    &lt;li&gt;&lt;b&gt;Target Encoding: &lt;/b&gt;Target encoding replaces each category with the mean of the target variable for that category. This can be useful when there is a relationship between the categorical variable and the target variable.&lt;/li&gt; 
&lt;/ol&gt; 
</span><span class="s0">#%% md 
</span><span class="s2">&lt;h2&gt;12-Validation, Test and Train sets&lt;/h2&gt; 
&lt;p&gt;there are some methods to split data to test and train:&lt;/p&gt; 
&lt;ul&gt; 
    &lt;li&gt;Randomly split the dataset into train and test sets: This method is the most common method. But it has a problem. If we split the dataset randomly, the train and test sets may not have the same distribution.&lt;/li&gt; 
    &lt;li&gt;Split the dataset based on the time: This method is useful when we have a time series dataset. But it is not useful in this case&lt;/li&gt; 
    &lt;li&gt;Split the dataset based on the target: This method is useful when we have an imbalanced dataset.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;strong&gt;Here we use first one and divide 80% train and 20% test.&lt;/strong&gt; 
</span><span class="s0">#%% md 
</span><span class="s2">&lt;p&gt;The &lt;strong&gt;random_state&lt;/strong&gt; is the seed used by the random number generator. It is used to make the results reproducible. If we don't set the random_state, the results will be different every time we run the code.&lt;/p&gt; 
&lt;h2&gt;Validation&lt;/h2&gt; 
&lt;p&gt;Validation set is used to evaluate the performance of the model and prevent overfitting. We usually use this set to tune the hyperparameters of the model.&lt;/p&gt; 
</span><span class="s0">#%% md 
</span><span class="s2">&lt;hr/&gt; 
&lt;h1&gt;Part3: Train, Testing and Evaluating the Models&lt;/h1&gt; 
&lt;h2&gt;14-super and unsupervised learning&lt;/h2&gt; 
&lt;ol&gt; 
    &lt;li&gt;&lt;b&gt;Unsupervised Models: &lt;/b&gt;In these models, data is trained without any labels or supervision. Unsupervised learning models include clustering and data mining algorithms. Example: K-Means &lt;/li&gt; 
    &lt;li&gt;&lt;b&gt;Semi-Supervised Models: &lt;/b&gt;In these models, only some of the data has labels or supervision, while the rest is unlabeled. These models are typically used when labeled data is scarce. Example: Label propagation&lt;/li&gt; 
    &lt;li&gt;&lt;b&gt;Reinforcement Learning Models: &lt;/b&gt; In these models, the model learns by interacting with an environment, receiving rewards and penalties. They are usually used for problems where we need to make decisions in a dynamic and variable environment. Example: Training robots &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;15-what is Regression?&lt;/h2&gt; 
&lt;p&gt;Regression refers to a class of machine learning algorithms used for predicting a continuous variable based on other input features. This variable can be a dependent variable, response, or predicted variable. The main difference between regression and classification methods is that in regression, the dependent variable or response is continuous, whereas in classification, the response variable is discrete or categorical.&lt;/p&gt; 
&lt;h2&gt;16-Regression formula&lt;/h2&gt; 
&lt;p&gt;The formula for linear regression in its simplest form (with one independent variable) is:&lt;/p&gt; 
&lt;p&gt;y = B&lt;sub&gt;0&lt;/sub&gt; + B&lt;sub&gt;1&lt;/sub&gt;X&lt;/p&gt; 
&lt;p&gt;In this equation:&lt;/p&gt; 
&lt;ul&gt; 
  &lt;li&gt;&lt;em&gt;Y&lt;/em&gt; is the dependent variable (the variable we are trying to predict).&lt;/li&gt; 
  &lt;li&gt;&lt;em&gt;X&lt;/em&gt; is the independent variable (the variable that we are using to make predictions).&lt;/li&gt; 
  &lt;li&gt;&lt;em&gt;B&lt;sub&gt;0&lt;/sub&gt;&lt;/em&gt; is the intercept, which represents the value of &lt;em&gt;Y&lt;/em&gt; when &lt;em&gt;X&lt;/em&gt; is zero.&lt;/li&gt; 
  &lt;li&gt;&lt;em&gt;B&lt;sub&gt;1&lt;/sub&gt;&lt;/em&gt; is the slope coefficient, which represents the change in &lt;em&gt;Y&lt;/em&gt; for a one-unit change in &lt;em&gt;X&lt;/em&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The goal of linear regression is to find the values of B&lt;sub&gt;0&lt;/sub&gt; and B&lt;sub&gt;1&lt;/sub&gt; that minimize the difference between the actual values of &lt;em&gt;Y&lt;/em&gt; and the values predicted by the linear equation. This is typically done using a method called least squares, which minimizes the sum of the squared differences between the observed and predicted values of &lt;em&gt;Y&lt;/em&gt;.&lt;/p&gt; 
 
</span><span class="s0">#%% md 
</span><span class="s2"># Linear Regression 
</span><span class="s0">#%% md 
    </span><span class="s2">Main form of simple linear regression function: 
$$f(x) = \alpha x + \beta$$ 
 
here we want to find the bias ($\alpha$) and slope($\beta$) by minimizing the derivation of the Residual Sum of Squares (RSS) function: 
 
- step 1: Compute RSS of the training data   
 
$$ RSS = \Sigma (y_i - (\hat{\beta} + \hat{\alpha} * x_i) )^2 $$ 
 
- step 2: Compute the derivatives of the RSS function in terms of $\alpha$ and $\beta$, and set them equal to 0 to find the desired parameters 
 
$$ \frac{\partial RSS}{\partial \beta} = \Sigma (-f(x_i) + \hat{\beta} + \hat{\alpha} * x_i) = 0$$ 
$$ \to \beta = \hat{y} - \hat{\alpha} \hat{x} \to (1)$$ 
 
 
$$ \frac{\partial RSS}{\partial \alpha} = \Sigma (-2 x_i y_i + 2 \hat{\beta} x_i + 2\hat{\alpha} x_i ^ 2) = 0 \to (2)$$ 
 
$$ (1) , (2) \to \hat{\alpha} = \frac{\Sigma{(x_i - \hat{x})(y_i - \hat{y})}}{\Sigma{(x_i - \hat{x})^2}} 
$$ 
$$ \hat{\beta} = y - \hat{a} x$$ 
 
 
</span><span class="s0">#%% md 
</span><span class="s2">Based on the above formula, implement the function below to compute the parameters of a simple linear regression 
</span><span class="s0">#%% 
</span><span class="s1">def </span><span class="s2">linear_regression</span><span class="s3">(</span><span class="s2">input</span><span class="s3">, </span><span class="s2">output</span><span class="s3">):</span>
  <span class="s2">x_mean </span><span class="s3">= </span><span class="s2">np</span><span class="s3">.</span><span class="s2">mean</span><span class="s3">(</span><span class="s2">input</span><span class="s3">)</span>
  <span class="s2">y_mean </span><span class="s3">= </span><span class="s2">np</span><span class="s3">.</span><span class="s2">mean</span><span class="s3">(</span><span class="s2">output</span><span class="s3">)</span>
  
  <span class="s2">numerator </span><span class="s3">= </span><span class="s2">np</span><span class="s3">.</span><span class="s2">sum</span><span class="s3">((</span><span class="s2">input </span><span class="s3">- </span><span class="s2">x_mean</span><span class="s3">)*(</span><span class="s2">output </span><span class="s3">- </span><span class="s2">y_mean</span><span class="s3">))</span>
  <span class="s2">denominator </span><span class="s3">= </span><span class="s2">np</span><span class="s3">.</span><span class="s2">sum</span><span class="s3">((</span><span class="s2">input </span><span class="s3">- </span><span class="s2">x_mean</span><span class="s3">)**</span><span class="s5">2</span><span class="s3">)</span>
  
  <span class="s2">slope </span><span class="s3">= </span><span class="s2">numerator</span><span class="s3">/</span><span class="s2">denominator</span>
  <span class="s2">intercept </span><span class="s3">= </span><span class="s2">y_mean </span><span class="s3">- (</span><span class="s2">slope </span><span class="s3">* </span><span class="s2">x_mean</span><span class="s3">)</span>
  
  <span class="s1">return </span><span class="s2">slope</span><span class="s3">,</span><span class="s2">intercept</span>

  
<span class="s0">#%% md 
</span><span class="s2">Now complete this `get_regression_predictions(...)` function to predict the value of given data based on the calculated intercept and slope 
</span><span class="s0">#%% 
</span><span class="s1">def </span><span class="s2">get_regression_predictions</span><span class="s3">(</span><span class="s2">input</span><span class="s3">, </span><span class="s2">intercept</span><span class="s3">, </span><span class="s2">slope</span><span class="s3">):</span>
    <span class="s2">prediction </span><span class="s3">= (</span><span class="s2">slope </span><span class="s3">* </span><span class="s2">input</span><span class="s3">) + </span><span class="s2">intercept</span>
    <span class="s1">return </span><span class="s2">prediction</span>


<span class="s0">#%% md 
</span><span class="s2">Now that we have a model and can make predictions, let's evaluate our model using Root Mean Square Error (RMSE). RMSE is the square root of the mean of the squared differences between the residuals, and the residuals is just a fancy word for the difference between the predicted output and the true output. 
 
Complete the following function to compute the RSME of a simple linear regression model given the input_feature, output, intercept and slope: 
</span><span class="s0">#%% 
</span><span class="s1">def </span><span class="s2">get_root_mean_square_error</span><span class="s3">(</span><span class="s2">predicted_values</span><span class="s3">, </span><span class="s2">actual_values</span><span class="s3">):</span>
  <span class="s2">squared </span><span class="s3">= (</span><span class="s2">predicted_values </span><span class="s3">- </span><span class="s2">actual_values</span><span class="s3">)**</span><span class="s5">2</span>
  <span class="s2">mean_squared </span><span class="s3">= </span><span class="s2">np</span><span class="s3">.</span><span class="s2">mean</span><span class="s3">(</span><span class="s2">squared</span><span class="s3">)</span>
  <span class="s2">root_mean_squared_error </span><span class="s3">= </span><span class="s2">np</span><span class="s3">.</span><span class="s2">sqrt</span><span class="s3">(</span><span class="s2">mean_squared</span><span class="s3">)</span>
  <span class="s1">return </span><span class="s2">root_mean_squared_error</span>

<span class="s2">inp </span><span class="s3">= </span><span class="s2">np</span><span class="s3">.</span><span class="s2">array</span><span class="s3">([</span><span class="s5">11</span><span class="s3">,</span><span class="s5">2</span><span class="s3">,</span><span class="s5">5</span><span class="s3">,</span><span class="s5">4</span><span class="s3">,</span><span class="s5">3</span><span class="s3">,</span><span class="s5">9</span><span class="s3">,</span><span class="s5">1</span><span class="s3">])</span>
<span class="s2">out </span><span class="s3">= </span><span class="s2">np</span><span class="s3">.</span><span class="s2">array</span><span class="s3">([</span><span class="s5">4</span><span class="s3">,</span><span class="s5">12</span><span class="s3">,</span><span class="s5">10</span><span class="s3">,</span><span class="s5">5</span><span class="s3">,</span><span class="s5">3</span><span class="s3">,</span><span class="s5">0</span><span class="s3">,</span><span class="s5">7</span><span class="s3">])</span>
<span class="s2">a</span><span class="s3">,</span><span class="s2">b </span><span class="s3">= </span><span class="s2">linear_regression</span><span class="s3">(</span><span class="s2">inp</span><span class="s3">,</span><span class="s2">out</span><span class="s3">)</span>
<span class="s2">pre </span><span class="s3">= </span><span class="s2">get_regression_predictions</span><span class="s3">(</span><span class="s2">inp</span><span class="s3">,</span><span class="s2">b</span><span class="s3">,</span><span class="s2">a</span><span class="s3">)</span>

<span class="s2">print</span><span class="s3">(</span><span class="s2">get_root_mean_square_error</span><span class="s3">(</span><span class="s2">pre</span><span class="s3">,</span><span class="s2">out</span><span class="s3">))</span>
<span class="s0">#%% md 
</span><span class="s2">The RMSE has no bound, thus it becomes challenging to determine whether a particular RMSE value is considered good or bad without any reference point. Instead, we use R2 score. The R2 score is calculated by comparing the sum of the squared differences between the actual and predicted values of the dependent variable to the total sum of squared differences between the actual and mean values of the dependent variable. The R2 score is formulated as below: 
 
$$R^2 = 1 - \frac{SSres}{SStot} = 1 - \frac{\sum_{i=1}^{n} (y_{i,true} - y_{i,pred})^2}{\sum_{i=1}^{n} (y_{i,true} - \bar{y}_{true})^2} $$ 
</span><span class="s0">#%% md 
</span><span class="s2">Complete the following function to calculate the R2 score of a given input_feature, output, bias, and slope: 
</span><span class="s0">#%% 
</span><span class="s1">def </span><span class="s2">get_r2_score</span><span class="s3">(</span><span class="s2">predicted_values</span><span class="s3">, </span><span class="s2">actual_values</span><span class="s3">):</span>
  <span class="s2">actual_mean </span><span class="s3">= </span><span class="s2">np</span><span class="s3">.</span><span class="s2">mean</span><span class="s3">(</span><span class="s2">actual_values</span><span class="s3">)</span>
  <span class="s2">residual_sum </span><span class="s3">= </span><span class="s2">np</span><span class="s3">.</span><span class="s2">sum</span><span class="s3">((</span><span class="s2">actual_values </span><span class="s3">- </span><span class="s2">predicted_values</span><span class="s3">)**</span><span class="s5">2</span><span class="s3">)</span>
  <span class="s2">total_sum </span><span class="s3">= </span><span class="s2">np</span><span class="s3">.</span><span class="s2">sum</span><span class="s3">((</span><span class="s2">actual_values </span><span class="s3">- </span><span class="s2">actual_mean</span><span class="s3">)**</span><span class="s5">2</span><span class="s3">)</span>
  <span class="s2">r2_square </span><span class="s3">= </span><span class="s5">1 </span><span class="s3">- (</span><span class="s2">residual_sum</span><span class="s3">/</span><span class="s2">total_sum</span><span class="s3">)</span>
  <span class="s1">return </span><span class="s2">r2_square</span>

<span class="s2">inp </span><span class="s3">= </span><span class="s2">np</span><span class="s3">.</span><span class="s2">array</span><span class="s3">([</span><span class="s5">11</span><span class="s3">,</span><span class="s5">2</span><span class="s3">,</span><span class="s5">5</span><span class="s3">,</span><span class="s5">4</span><span class="s3">,</span><span class="s5">3</span><span class="s3">,</span><span class="s5">9</span><span class="s3">,</span><span class="s5">1</span><span class="s3">])</span>
<span class="s2">out </span><span class="s3">= </span><span class="s2">np</span><span class="s3">.</span><span class="s2">array</span><span class="s3">([</span><span class="s5">4</span><span class="s3">,</span><span class="s5">12</span><span class="s3">,</span><span class="s5">10</span><span class="s3">,</span><span class="s5">5</span><span class="s3">,</span><span class="s5">3</span><span class="s3">,</span><span class="s5">0</span><span class="s3">,</span><span class="s5">7</span><span class="s3">])</span>
<span class="s2">a</span><span class="s3">,</span><span class="s2">b </span><span class="s3">= </span><span class="s2">linear_regression</span><span class="s3">(</span><span class="s2">inp</span><span class="s3">,</span><span class="s2">out</span><span class="s3">)</span>
<span class="s2">pre </span><span class="s3">= </span><span class="s2">get_regression_predictions</span><span class="s3">(</span><span class="s2">inp</span><span class="s3">,</span><span class="s2">b</span><span class="s3">,</span><span class="s2">a</span><span class="s3">)</span>

<span class="s2">print</span><span class="s3">(</span><span class="s2">get_r2_score</span><span class="s3">(</span><span class="s2">pre</span><span class="s3">,</span><span class="s2">out</span><span class="s3">))</span>

<span class="s0">#%% md 
</span><span class="s2">Now calculate the fitness of the model. 
Remember to provide explanation for the outputs in your code! 
</span><span class="s0">#%% 
</span><span class="s2">designated_feature_list </span><span class="s3">= </span><span class="s2">imputed</span><span class="s3">.</span><span class="s2">columns</span><span class="s3">[:-</span><span class="s5">1</span><span class="s3">]</span>
<span class="s2">output </span><span class="s3">= </span><span class="s2">imputed</span><span class="s3">[</span><span class="s4">&quot;MEDV&quot;</span><span class="s3">].</span><span class="s2">values</span>

<span class="s1">for </span><span class="s2">feature </span><span class="s1">in </span><span class="s2">designated_feature_list</span><span class="s3">:</span>
   
    <span class="s2">input_feature </span><span class="s3">= </span><span class="s2">imputed</span><span class="s3">[</span><span class="s2">feature</span><span class="s3">].</span><span class="s2">values</span>
    
    <span class="s2">slope</span><span class="s3">,</span><span class="s2">intercept </span><span class="s3">= </span><span class="s2">linear_regression</span><span class="s3">(</span><span class="s2">input_feature</span><span class="s3">, </span><span class="s2">output</span><span class="s3">)</span>
    
    <span class="s2">predicted_values </span><span class="s3">= </span><span class="s2">get_regression_predictions</span><span class="s3">(</span><span class="s2">input_feature</span><span class="s3">, </span><span class="s2">intercept</span><span class="s3">, </span><span class="s2">slope</span><span class="s3">)</span>
    
    <span class="s2">root_square_mean_error </span><span class="s3">= </span><span class="s2">get_root_mean_square_error</span><span class="s3">(</span><span class="s2">predicted_values</span><span class="s3">, </span><span class="s2">output</span><span class="s3">)</span>
    
    <span class="s2">r2_score </span><span class="s3">= </span><span class="s2">get_r2_score</span><span class="s3">(</span><span class="s2">predicted_values</span><span class="s3">, </span><span class="s2">output</span><span class="s3">)</span>
    
    <span class="s2">print</span><span class="s3">(</span><span class="s4">&quot;Feature:&quot;</span><span class="s3">, </span><span class="s2">feature</span><span class="s3">)</span>
    <span class="s2">print</span><span class="s3">(</span><span class="s4">&quot;RMSE:&quot;</span><span class="s3">, </span><span class="s2">root_square_mean_error</span><span class="s3">)</span>
    <span class="s2">print</span><span class="s3">(</span><span class="s4">&quot;R2 Score:&quot;</span><span class="s3">, </span><span class="s2">r2_score</span><span class="s3">)</span>
    <span class="s2">print</span><span class="s3">(</span><span class="s4">&quot;---------------------&quot;</span><span class="s3">)</span>
      

<span class="s0">#%% md 
</span><span class="s2">&lt;h2&gt;17-which feature?&lt;/h2&gt; 
&lt;p&gt;Since the &lt;span style=&quot;color:blue&quot;&gt;LSTAT&lt;/span&gt; has a more linear relationship with the target than the rest of the features, it is more suitable for use here in linear Regression method&lt;/p&gt; 
</span><span class="s0">#%% md 
</span><span class="s2">## 18-Validation identifier 
 
- **MSE:** Mean Squared Error (MSE) is a common measure of the average squared difference between the predicted and actual values in a regression problem. 
 
$$ \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 $$ 
 
- **RMSE:** Root Mean Squared Error (RMSE) is the square root of the Mean Squared Error (MSE). 
 
$$ \text{RMSE} = \sqrt{\text{MSE}} $$ 
 
- **R2_Score:** R-squared (R2) Score, also known as the coefficient of determination, measures the proportion of the variance in the dependent variable that is explained by the independent variables in the model. 
 
$$ R^2 = 1 - \frac{\text{RSS}}{\text{TSS}} $$ 
 
- **RSS:** Residual Sum of Squares (RSS) is the sum of the squared differences between the actual values and the predicted values. 
 
$$ \text{RSS} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 $$ 
 
</span><span class="s0">#%% md 
</span><span class="s2">&lt;h2&gt;20-Plotting Scatter for model validation&lt;/h2&gt; 
</span><span class="s0">#%% 
</span><span class="s2">input </span><span class="s3">= </span><span class="s2">imputed</span><span class="s3">[</span><span class="s4">&quot;LSTAT&quot;</span><span class="s3">].</span><span class="s2">values</span>
    
<span class="s2">slope</span><span class="s3">,</span><span class="s2">intercept </span><span class="s3">= </span><span class="s2">linear_regression</span><span class="s3">(</span><span class="s2">input</span><span class="s3">, </span><span class="s2">imputed</span><span class="s3">[</span><span class="s4">&quot;MEDV&quot;</span><span class="s3">].</span><span class="s2">values</span><span class="s3">)</span>
    
<span class="s2">predicted_values </span><span class="s3">= </span><span class="s2">get_regression_predictions</span><span class="s3">(</span><span class="s2">input</span><span class="s3">, </span><span class="s2">intercept</span><span class="s3">, </span><span class="s2">slope</span><span class="s3">)</span>

<span class="s2">plt</span><span class="s3">.</span><span class="s2">scatter</span><span class="s3">(</span><span class="s2">imputed</span><span class="s3">[</span><span class="s4">&quot;MEDV&quot;</span><span class="s3">], </span><span class="s2">predicted_values</span><span class="s3">, </span><span class="s2">color</span><span class="s3">=</span><span class="s4">'blue'</span><span class="s3">, </span><span class="s2">label</span><span class="s3">=</span><span class="s4">'Actual vs Predicted'</span><span class="s3">)</span>

<span class="s2">x_line </span><span class="s3">= [</span><span class="s2">min</span><span class="s3">(</span><span class="s2">imputed</span><span class="s3">[</span><span class="s4">&quot;MEDV&quot;</span><span class="s3">].</span><span class="s2">values</span><span class="s3">), </span><span class="s2">max</span><span class="s3">(</span><span class="s2">imputed</span><span class="s3">[</span><span class="s4">&quot;MEDV&quot;</span><span class="s3">].</span><span class="s2">values</span><span class="s3">)]</span>
<span class="s2">plt</span><span class="s3">.</span><span class="s2">plot</span><span class="s3">(</span><span class="s2">x_line</span><span class="s3">, </span><span class="s2">x_line</span><span class="s3">, </span><span class="s2">color</span><span class="s3">=</span><span class="s4">'red'</span><span class="s3">, </span><span class="s2">label</span><span class="s3">=</span><span class="s4">'y = x'</span><span class="s3">)</span>

<span class="s2">plt</span><span class="s3">.</span><span class="s2">xlabel</span><span class="s3">(</span><span class="s4">'Actual Values'</span><span class="s3">)</span>
<span class="s2">plt</span><span class="s3">.</span><span class="s2">ylabel</span><span class="s3">(</span><span class="s4">'Predicted Values'</span><span class="s3">)</span>
<span class="s2">plt</span><span class="s3">.</span><span class="s2">title</span><span class="s3">(</span><span class="s4">'Actual vs Predicted Values'</span><span class="s3">)</span>

<span class="s2">plt</span><span class="s3">.</span><span class="s2">legend</span><span class="s3">()</span>

<span class="s2">plt</span><span class="s3">.</span><span class="s2">show</span><span class="s3">()</span>

<span class="s0">#%% md 
</span><span class="s2">&lt;h1&gt;Part3: Classification&lt;/h1&gt; 
&lt;h2&gt;21-decision tree&lt;/h2&gt; 
&lt;p&gt;Pruning in decision trees is a technique used to reduce the size of the tree by removing nodes that are not providing significant predictive power or are likely to be overfitting the training data. Pruning helps prevent overfitting, which occurs when the decision tree captures noise or outliers in the training data, leading to poor generalization to new, unseen data.&lt;/p&gt; 
&lt;h3&gt;Advantages&lt;/h3&gt; 
    &lt;ul&gt; 
        &lt;li&gt;Prevents Overfitting&lt;/li&gt; 
        &lt;li&gt;Reduces Computational Complexity&lt;/li&gt; 
    &lt;/ul&gt; 
&lt;h3&gt;disAdvantages&lt;/h3&gt; 
    &lt;ul&gt; 
        &lt;li&gt;Loss of Information&lt;/li&gt; 
        &lt;li&gt;Potential Underfitting&lt;/li&gt; 
    &lt;/ul&gt; 
&lt;h2&gt;22-Using decision tree&lt;/h2&gt; 
    &lt;p&gt;Using decision trees may have advantages over other models in the following situations:&lt;/p&gt;     
    &lt;ul&gt; 
        &lt;li&gt;Interpretability: Decision trees are highly interpretable due to their simple and intuitive structure. They can be useful for understanding patterns in data and can be used for decision-making and strategic purposes.&lt;/li&gt; 
        &lt;li&gt;Handling multiple factors: Decision trees can examine various factors and their relationships with the response variable without making prior assumptions about the distribution of data or linear relationships.&lt;/li&gt; 
        &lt;li&gt;Non-linear relationships: Decision trees can accommodate non-linear patterns in the data, such as cases where the relationship between features and the response variable is not linear.&lt;/li&gt; 
    &lt;/ul&gt; 
&lt;h2&gt;23-KNN different&lt;/h2&gt; 
    &lt;p&gt;there are some markable different between KNN and other classification methods:&lt;/p&gt; 
     &lt;ul&gt; 
        &lt;li&gt;KNN is an instance-based learning algorithm, meaning it does not explicitly learn a model during training. Instead, it memorizes the entire training dataset and classifies new instances based on their similarity to existing instances in the feature space.&lt;/li&gt; 
        &lt;li&gt;KNN determines the class of a new instance based on the majority class of its k nearest neighbors in the feature space. The decision boundary in KNN is flexible and can be highly non-linear.&lt;/li&gt; 
        &lt;li&gt;KNN can handle missing values by imputing them based on the values of the nearest neighbors.&lt;/li&gt; 
        &lt;li&gt;KNN makes predictions based on local information, considering only the k nearest neighbors of each instance. It does not attempt to learn global patterns or relationships in the data.&lt;/li&gt; 
    &lt;/ul&gt; 
 
</span><span class="s0">#%% md 
</span><span class="s2">&lt;h2&gt;24-One nearest neighbour&lt;/h2&gt; 
&lt;p&gt;it is likely work as KNN with only comparing the class of nearet neighbour.&lt;/p&gt; 
&lt;b&gt;Advantage:&lt;/b&gt; 
&lt;p&gt;Simplicity-Non-parametric&lt;/p&gt; 
&lt;b&gt;disAdvantage:&lt;/b&gt; 
&lt;p&gt;Sensitive to Noisy Data-Scaling with Dimensionality&lt;/p&gt; 
</span><span class="s0">#%% md 
</span><span class="s2">&lt;h2&gt;25-other KNN distance functions&lt;/h2&gt; 
&lt;ol&gt; 
    &lt;li&gt;Euclidean Distance&lt;/li&gt; 
    &lt;p&gt;$$d(\mathbf{p}, \mathbf{q}) = \sqrt{\sum_{i=1}^{n} (q_i - p_i)^2}$$&lt;/p&gt; 
    &lt;li&gt;Manhattan Distance&lt;/li&gt; 
    &lt;p&gt;$$d(\mathbf{p}, \mathbf{q}) = \sum_{i=1}^{n} |q_i - p_i|$$&lt;/p&gt; 
    &lt;li&gt;Minkowski Distance&lt;/li&gt; 
    &lt;p&gt;$$d(\mathbf{p}, \mathbf{q}) = \left( \sum_{i=1}^{n} |q_i - p_i|^p \right)^{\frac{1}{p}}$$&lt;/p&gt; 
&lt;/ol&gt; 
</span><span class="s0">#%% md 
</span><span class="s2">&lt;h2&gt;26-implement Decision tree and KNN&lt;/h2&gt; 
</span><span class="s0">#%% 
</span><span class="s2">deciles </span><span class="s3">= </span><span class="s2">imputed</span><span class="s3">[</span><span class="s4">'MEDV'</span><span class="s3">].</span><span class="s2">quantile</span><span class="s3">([</span><span class="s5">0.2</span><span class="s3">, </span><span class="s5">0.4</span><span class="s3">, </span><span class="s5">0.6</span><span class="s3">, </span><span class="s5">0.8</span><span class="s3">])</span>

<span class="s2">luxury_threshold </span><span class="s3">= </span><span class="s2">deciles</span><span class="s3">.</span><span class="s2">iloc</span><span class="s3">[-</span><span class="s5">1</span><span class="s3">]</span>
<span class="s2">economic_threshold </span><span class="s3">= </span><span class="s2">deciles</span><span class="s3">.</span><span class="s2">iloc</span><span class="s3">[</span><span class="s5">0</span><span class="s3">]</span>

<span class="s2">imputed</span><span class="s3">[</span><span class="s4">'HCAT'</span><span class="s3">] = </span><span class="s4">&quot;ordinary&quot;</span>
<span class="s2">imputed</span><span class="s3">.</span><span class="s2">loc</span><span class="s3">[</span><span class="s2">imputed</span><span class="s3">[</span><span class="s4">'MEDV'</span><span class="s3">] &gt;= </span><span class="s2">luxury_threshold</span><span class="s3">, </span><span class="s4">'HCAT'</span><span class="s3">] = </span><span class="s4">&quot;luxury&quot;</span>
<span class="s2">imputed</span><span class="s3">.</span><span class="s2">loc</span><span class="s3">[</span><span class="s2">imputed</span><span class="s3">[</span><span class="s4">'MEDV'</span><span class="s3">] &lt;= </span><span class="s2">economic_threshold</span><span class="s3">, </span><span class="s4">'HCAT'</span><span class="s3">] = </span><span class="s4">&quot;economy&quot;</span>

<span class="s2">dt_classifier </span><span class="s3">= </span><span class="s2">DecisionTreeClassifier</span><span class="s3">(</span><span class="s2">max_depth</span><span class="s3">=</span><span class="s1">None</span><span class="s3">)</span>
<span class="s2">knn_classifier </span><span class="s3">= </span><span class="s2">KNeighborsClassifier</span><span class="s3">(</span><span class="s2">n_neighbors</span><span class="s3">=</span><span class="s5">8 </span><span class="s3">,</span><span class="s2">metric</span><span class="s3">=</span><span class="s4">&quot;manhattan&quot;</span><span class="s3">,</span><span class="s2">weights</span><span class="s3">=</span><span class="s4">&quot;distance&quot;</span><span class="s3">)</span>

<span class="s2">X_train </span><span class="s3">, </span><span class="s2">X_test </span><span class="s3">, </span><span class="s2">y_train </span><span class="s3">, </span><span class="s2">y_test </span><span class="s3">= </span><span class="s2">train_test_split</span><span class="s3">(</span><span class="s2">imputed</span><span class="s3">.</span><span class="s2">drop</span><span class="s3">(</span><span class="s4">'HCAT'</span><span class="s3">, </span><span class="s2">axis</span><span class="s3">=</span><span class="s5">1</span><span class="s3">), </span><span class="s2">imputed</span><span class="s3">[</span><span class="s4">'HCAT'</span><span class="s3">], </span><span class="s2">test_size</span><span class="s3">=</span><span class="s5">0.2</span><span class="s3">, </span><span class="s2">random_state</span><span class="s3">=</span><span class="s5">42</span><span class="s3">)</span>

<span class="s2">dt_classifier</span><span class="s3">.</span><span class="s2">fit</span><span class="s3">(</span><span class="s2">X_train</span><span class="s3">,</span><span class="s2">y_train</span><span class="s3">)</span>
<span class="s2">knn_classifier</span><span class="s3">.</span><span class="s2">fit</span><span class="s3">(</span><span class="s2">X_train</span><span class="s3">,</span><span class="s2">y_train</span><span class="s3">)</span>

<span class="s2">dt_predict </span><span class="s3">= </span><span class="s2">dt_classifier</span><span class="s3">.</span><span class="s2">predict</span><span class="s3">(</span><span class="s2">X_test</span><span class="s3">)</span>
<span class="s2">knn_predict </span><span class="s3">= </span><span class="s2">knn_classifier</span><span class="s3">.</span><span class="s2">predict</span><span class="s3">(</span><span class="s2">X_test</span><span class="s3">)</span>
<span class="s2">dt_accuracy </span><span class="s3">= </span><span class="s2">accuracy_score</span><span class="s3">(</span><span class="s2">y_test</span><span class="s3">,</span><span class="s2">dt_predict</span><span class="s3">)</span>
<span class="s2">knn_accuracy </span><span class="s3">= </span><span class="s2">accuracy_score</span><span class="s3">(</span><span class="s2">y_test</span><span class="s3">,</span><span class="s2">knn_predict</span><span class="s3">)</span>

<span class="s2">print</span><span class="s3">(</span><span class="s4">&quot;Decision tree Accuracy: &quot;</span><span class="s3">,</span><span class="s2">dt_accuracy</span><span class="s3">)</span>
<span class="s2">print</span><span class="s3">(</span><span class="s4">&quot;KNN Accuracy: &quot;</span><span class="s3">,</span><span class="s2">knn_accuracy</span><span class="s3">)</span>
<span class="s0">#%% md 
</span><span class="s2">&lt;h2&gt;27-GridSearchCV&lt;/h2&gt; 
</span><span class="s0">#%% 
</span><span class="s2">dt_param_grid </span><span class="s3">= {</span><span class="s4">'max_depth'</span><span class="s3">: [</span><span class="s1">None</span><span class="s3">, </span><span class="s5">10</span><span class="s3">, </span><span class="s5">20</span><span class="s3">, </span><span class="s5">30</span><span class="s3">],</span>
                 <span class="s4">'min_samples_split'</span><span class="s3">: [</span><span class="s5">2</span><span class="s3">, </span><span class="s5">5</span><span class="s3">, </span><span class="s5">10</span><span class="s3">],</span>
                 <span class="s4">'min_samples_leaf'</span><span class="s3">: [</span><span class="s5">1</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s5">4</span><span class="s3">]}</span>

<span class="s2">knn_param_grid </span><span class="s3">= {</span><span class="s4">'n_neighbors'</span><span class="s3">: [</span><span class="s5">3</span><span class="s3">, </span><span class="s5">5</span><span class="s3">, </span><span class="s5">7</span><span class="s3">, </span><span class="s5">9</span><span class="s3">],</span>
                  <span class="s4">'weights'</span><span class="s3">: [</span><span class="s4">'uniform'</span><span class="s3">, </span><span class="s4">'distance'</span><span class="s3">],</span>
                  <span class="s4">'metric'</span><span class="s3">: [</span><span class="s4">'euclidean'</span><span class="s3">, </span><span class="s4">'manhattan'</span><span class="s3">]}</span>

<span class="s2">dt_grid_search </span><span class="s3">= </span><span class="s2">GridSearchCV</span><span class="s3">(</span><span class="s2">dt_classifier</span><span class="s3">, </span><span class="s2">dt_param_grid</span><span class="s3">, </span><span class="s2">cv</span><span class="s3">=</span><span class="s5">5</span><span class="s3">)</span>
<span class="s2">dt_grid_search</span><span class="s3">.</span><span class="s2">fit</span><span class="s3">(</span><span class="s2">X_train</span><span class="s3">, </span><span class="s2">y_train</span><span class="s3">)</span>
<span class="s2">knn_grid_search </span><span class="s3">= </span><span class="s2">GridSearchCV</span><span class="s3">(</span><span class="s2">knn_classifier</span><span class="s3">, </span><span class="s2">knn_param_grid</span><span class="s3">, </span><span class="s2">cv</span><span class="s3">=</span><span class="s5">5</span><span class="s3">)</span>
<span class="s2">knn_grid_search</span><span class="s3">.</span><span class="s2">fit</span><span class="s3">(</span><span class="s2">X_train</span><span class="s3">, </span><span class="s2">y_train</span><span class="s3">)</span>

<span class="s2">print</span><span class="s3">(</span><span class="s4">&quot;Best Hyperparameters for Decision Tree:&quot;</span><span class="s3">, </span><span class="s2">dt_grid_search</span><span class="s3">.</span><span class="s2">best_params_</span><span class="s3">)</span>
<span class="s2">print</span><span class="s3">(</span><span class="s4">&quot;Best Hyperparameters for KNN:&quot;</span><span class="s3">, </span><span class="s2">knn_grid_search</span><span class="s3">.</span><span class="s2">best_params_</span><span class="s3">)</span>
<span class="s0">#%% md 
</span><span class="s2">&lt;h2&gt;28-Plotting decision tree&lt;/h2&gt; 
</span><span class="s0">#%% 
</span><span class="s2">plt</span><span class="s3">.</span><span class="s2">figure</span><span class="s3">(</span><span class="s2">figsize</span><span class="s3">=(</span><span class="s5">20</span><span class="s3">,</span><span class="s5">10</span><span class="s3">))</span>
<span class="s2">plot_tree</span><span class="s3">(</span><span class="s2">dt_classifier</span><span class="s3">,</span><span class="s2">filled</span><span class="s3">=</span><span class="s1">True</span><span class="s3">,</span><span class="s2">feature_names</span><span class="s3">=</span><span class="s2">imputed</span><span class="s3">[</span><span class="s4">&quot;HCAT&quot;</span><span class="s3">],</span><span class="s2">class_names</span><span class="s3">=[</span><span class="s4">&quot;ordinary&quot;</span><span class="s3">,</span><span class="s4">&quot;economy&quot;</span><span class="s3">,</span><span class="s4">&quot;luxury&quot;</span><span class="s3">])</span>
<span class="s2">plt</span><span class="s3">.</span><span class="s2">show</span><span class="s3">()</span>
<span class="s0">#%% md 
</span><span class="s2">&lt;h2&gt;29-Under and overfitting&lt;/h2&gt; 
&lt;p&gt;Underfitting and overfitting are two common problems in machine learning models that arise during the training process. They occur due to the model's inability to generalize well to unseen data.&lt;/p&gt; 
&lt;ol&gt; 
    &lt;li&gt;&lt;b&gt;Underfitting: &lt;/b&gt;Underfitting occurs when a model is too simple to capture the underlying patterns in the data.&lt;/li&gt; 
    &lt;li&gt;&lt;b&gt;Overfitting: &lt;/b&gt;Overfitting occurs when a model learns the training data too well, including the noise and fluctuations in the data.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;strong&gt;it seems in our model we haven't these two problem as we have enough number of data and also we don't have noise or irrelevant data&lt;/strong&gt; 
 
</span><span class="s0">#%% md 
</span><span class="s2">&lt;hr/&gt; 
&lt;h1&gt;Part4: Ensemble&lt;/h1&gt; 
&lt;h2&gt;30-why?&lt;/h2&gt; 
&lt;p&gt;Ensemble methods are used in machine learning for improving the performance and robustness of models by combining multiple base models. They can lead to better predictive performance compared to individual models by leveraging the wisdom of crowds and reducing the risk of overfitting. Here are some reasons why ensemble methods are used:&lt;/p&gt; 
&lt;ul&gt; 
    &lt;li&gt;Improved Performance&lt;/li&gt; 
    &lt;li&gt;Reduced Overfitting&lt;/li&gt; 
    &lt;li&gt;Versatility&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;31-Bagging and Boosting&lt;/h2&gt; 
&lt;p&gt;Bootstrap aggregating (bagging) involves training multiple base models on bootstrap samples of the training data and averaging their predictions.but  Boosting algorithms train base models sequentially, where each subsequent model focuses on the instances that were misclassified by the previous models.&lt;/p&gt; 
&lt;h2&gt;32-Random Forest&lt;/h2&gt; 
&lt;p&gt;Random Forest is an ensemble learning method used for classification, regression, and other tasks that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.&lt;/p&gt; 
&lt;ol&gt; 
    &lt;li&gt;Bootstrapping&lt;/li&gt; 
    &lt;li&gt;Decision Tree Construction&lt;/li&gt; 
    &lt;li&gt;Voting or Averaging&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;33-Bootstrapping&lt;/h2&gt; 
&lt;p&gt;as mentioned before,Bootstrapping in the context of Random Forest refers to the process of creating multiple subsets of the original training data by sampling with replacement. This process is used to introduce randomness and diversity among the decision trees in the forest&lt;/p&gt; 
&lt;h2&gt;34-number of trees&lt;/h2&gt; 
&lt;p&gt;certainly the number of trees in forest is effective on our time and memory complezity but by experiment is suggest that a random forest should have a number of trees between 64 - 128 trees.&lt;/p&gt; 
&lt;h2&gt;35-is it always usefull?&lt;/h2&gt; 
&lt;p&gt;Using Random Forest for machine learning problems is highly effective in many cases, but it's not always suitable.When your dataset is large and you're looking for a model with high accuracy and good performance on test data, using Random Forest may be very suitable.or When your problem is complex and the relationships between variables are intricate, Random Forest can improve the model's performance.&lt;b&gt;but&lt;/b&gt; If your data is relatively small,or where interpretability of the model is required using Random Forest may lead to models with lower accuracy.&lt;/p&gt; 
&lt;h2&gt;36-effect on Variance&lt;/h2&gt; 
&lt;p&gt;the Random Forest generally has bellow effects on dataset Variance:&lt;/p&gt; 
&lt;ul&gt; 
    &lt;li&gt;Reduction in Variance&lt;/li&gt; 
    &lt;li&gt;Balancing Variance and Standard Deviation&lt;/li&gt; 
    &lt;li&gt;Preservation (or Increase) in Variance&lt;/li&gt; 
&lt;/ul&gt; 
</span><span class="s0">#%% 
</span><span class="s2">rf_classifier </span><span class="s3">= </span><span class="s2">RandomForestClassifier</span><span class="s3">()</span>

<span class="s2">param_grid </span><span class="s3">= {</span>
    <span class="s4">'n_estimators'</span><span class="s3">: [</span><span class="s5">100</span><span class="s3">, </span><span class="s5">200</span><span class="s3">, </span><span class="s5">300</span><span class="s3">],</span>
    <span class="s4">'max_depth'</span><span class="s3">: [</span><span class="s1">None</span><span class="s3">, </span><span class="s5">5</span><span class="s3">, </span><span class="s5">10</span><span class="s3">, </span><span class="s5">20</span><span class="s3">],</span>
    <span class="s4">'min_samples_split'</span><span class="s3">: [</span><span class="s5">2</span><span class="s3">, </span><span class="s5">5</span><span class="s3">, </span><span class="s5">10</span><span class="s3">],</span>
    <span class="s4">'min_samples_leaf'</span><span class="s3">: [</span><span class="s5">1</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s5">4</span><span class="s3">],</span>
    <span class="s4">'max_features'</span><span class="s3">: [</span><span class="s4">'sqrt'</span><span class="s3">, </span><span class="s4">'log2'</span><span class="s3">,</span><span class="s1">None</span><span class="s3">]</span>
<span class="s3">}</span>

<span class="s2">grid_search </span><span class="s3">= </span><span class="s2">GridSearchCV</span><span class="s3">(</span><span class="s2">estimator</span><span class="s3">=</span><span class="s2">rf_classifier</span><span class="s3">, </span><span class="s2">param_grid</span><span class="s3">=</span><span class="s2">param_grid</span><span class="s3">, </span><span class="s2">cv</span><span class="s3">=</span><span class="s5">5</span><span class="s3">, </span><span class="s2">n_jobs</span><span class="s3">=-</span><span class="s5">1</span><span class="s3">,</span><span class="s2">error_score</span><span class="s3">=</span><span class="s4">&quot;raise&quot;</span><span class="s3">)</span>
<span class="s2">grid_search</span><span class="s3">.</span><span class="s2">fit</span><span class="s3">(</span><span class="s2">X_train</span><span class="s3">, </span><span class="s2">y_train</span><span class="s3">)</span>

<span class="s2">print</span><span class="s3">(</span><span class="s4">&quot;Best Hyperparameters:&quot;</span><span class="s3">, </span><span class="s2">grid_search</span><span class="s3">.</span><span class="s2">best_params_</span><span class="s3">)</span>

<span class="s2">best_rf_classifier </span><span class="s3">= </span><span class="s2">RandomForestClassifier</span><span class="s3">(**</span><span class="s2">grid_search</span><span class="s3">.</span><span class="s2">best_params_</span><span class="s3">)</span>
<span class="s2">best_rf_classifier</span><span class="s3">.</span><span class="s2">fit</span><span class="s3">(</span><span class="s2">X_train</span><span class="s3">, </span><span class="s2">y_train</span><span class="s3">)</span>
<span class="s0">#%% md 
</span><span class="s2">&lt;h2&gt;37-HyperParameters&lt;/h2&gt; 
&lt;ul&gt; 
    &lt;li&gt;&lt;b&gt;n_estimators: &lt;/b&gt;The number of decision trees in the forest&lt;/li&gt; 
    &lt;li&gt;&lt;b&gt;max_depth: &lt;/b&gt;The maximum depth of each decision tree in the forest&lt;/li&gt; 
    &lt;li&gt;&lt;b&gt;min_samples_split: &lt;/b&gt;The minimum number of samples required to split an internal node. &lt;/li&gt; 
    &lt;li&gt;&lt;b&gt;min_samples_leaf: &lt;/b&gt;The minimum number of samples required to be at a leaf node.&lt;/li&gt; 
    &lt;li&gt;&lt;b&gt;max_features: &lt;/b&gt;The number of features to consider when looking for the best split. &lt;/li&gt; 
&lt;/ul&gt; 
</span><span class="s0">#%% md 
</span><span class="s2">&lt;hr/&gt; 
&lt;h1&gt;Part5:SVM&lt;/h1&gt; 
</span><span class="s0">#%% 
</span><span class="s2">rbf_classifier </span><span class="s3">= </span><span class="s2">SVC</span><span class="s3">(</span><span class="s2">kernel</span><span class="s3">=</span><span class="s4">&quot;rbf&quot;</span><span class="s3">)</span>
<span class="s2">linear_classifier </span><span class="s3">= </span><span class="s2">SVC</span><span class="s3">(</span><span class="s2">kernel</span><span class="s3">=</span><span class="s4">&quot;linear&quot;</span><span class="s3">)</span>

<span class="s2">rbf_classifier</span><span class="s3">.</span><span class="s2">fit</span><span class="s3">(</span><span class="s2">X_train</span><span class="s3">,</span><span class="s2">y_train</span><span class="s3">)</span>
<span class="s2">linear_classifier</span><span class="s3">.</span><span class="s2">fit</span><span class="s3">(</span><span class="s2">X_train</span><span class="s3">,</span><span class="s2">y_train</span><span class="s3">)</span>
<span class="s2">rbf_predict </span><span class="s3">= </span><span class="s2">rbf_classifier</span><span class="s3">.</span><span class="s2">predict</span><span class="s3">(</span><span class="s2">X_test</span><span class="s3">)</span>
<span class="s2">linear_predict </span><span class="s3">= </span><span class="s2">linear_classifier</span><span class="s3">.</span><span class="s2">predict</span><span class="s3">(</span><span class="s2">X_test</span><span class="s3">)</span>
<span class="s2">rbf_confusion_matrix </span><span class="s3">= </span><span class="s2">confusion_matrix</span><span class="s3">(</span><span class="s2">y_test</span><span class="s3">,</span><span class="s2">rbf_predict</span><span class="s3">)</span>
<span class="s2">linear_confusion_matrix </span><span class="s3">= </span><span class="s2">confusion_matrix</span><span class="s3">(</span><span class="s2">y_test</span><span class="s3">,</span><span class="s2">linear_predict</span><span class="s3">)</span>

<span class="s2">rbf_accuracy </span><span class="s3">= </span><span class="s2">accuracy_score</span><span class="s3">(</span><span class="s2">y_test</span><span class="s3">, </span><span class="s2">rbf_predict</span><span class="s3">)</span>
<span class="s2">linear_accuracy </span><span class="s3">= </span><span class="s2">accuracy_score</span><span class="s3">(</span><span class="s2">y_test</span><span class="s3">, </span><span class="s2">linear_predict</span><span class="s3">)</span>
<span class="s2">rbf_report </span><span class="s3">= </span><span class="s2">classification_report</span><span class="s3">(</span><span class="s2">y_test</span><span class="s3">, </span><span class="s2">rbf_predict</span><span class="s3">,</span><span class="s2">zero_division</span><span class="s3">=</span><span class="s5">1</span><span class="s3">)</span>
<span class="s2">linear_report </span><span class="s3">= </span><span class="s2">classification_report</span><span class="s3">(</span><span class="s2">y_test</span><span class="s3">, </span><span class="s2">linear_predict</span><span class="s3">,</span><span class="s2">zero_division</span><span class="s3">=</span><span class="s5">1</span><span class="s3">)</span>

<span class="s2">print</span><span class="s3">(</span><span class="s4">&quot;RBF Kernel:&quot;</span><span class="s3">)</span>
<span class="s2">print</span><span class="s3">(</span><span class="s4">&quot;Confusion Matrix:</span><span class="s1">\n</span><span class="s4">&quot;</span><span class="s3">, </span><span class="s2">rbf_confusion_matrix</span><span class="s3">)</span>
<span class="s2">print</span><span class="s3">(</span><span class="s4">&quot;Accuracy:&quot;</span><span class="s3">, </span><span class="s2">rbf_accuracy</span><span class="s3">)</span>
<span class="s2">print</span><span class="s3">(</span><span class="s4">&quot;Classification Report:</span><span class="s1">\n</span><span class="s4">&quot;</span><span class="s3">, </span><span class="s2">rbf_report</span><span class="s3">)</span>

<span class="s2">print</span><span class="s3">(</span><span class="s4">&quot;</span><span class="s1">\n</span><span class="s4">Linear Kernel:&quot;</span><span class="s3">)</span>
<span class="s2">print</span><span class="s3">(</span><span class="s4">&quot;Confusion Matrix:</span><span class="s1">\n</span><span class="s4">&quot;</span><span class="s3">, </span><span class="s2">linear_confusion_matrix</span><span class="s3">)</span>
<span class="s2">print</span><span class="s3">(</span><span class="s4">&quot;Accuracy:&quot;</span><span class="s3">, </span><span class="s2">linear_accuracy</span><span class="s3">)</span>
<span class="s2">print</span><span class="s3">(</span><span class="s4">&quot;Classification Report:</span><span class="s1">\n</span><span class="s4">&quot;</span><span class="s3">, </span><span class="s2">linear_report</span><span class="s3">)</span>
<span class="s0">#%% 
</span><span class="s2">param_grid </span><span class="s3">= {</span>
    <span class="s4">'C'</span><span class="s3">: [</span><span class="s5">0.1</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">10</span><span class="s3">, </span><span class="s5">100</span><span class="s3">],</span>
    <span class="s4">'gamma'</span><span class="s3">: [</span><span class="s5">0.001</span><span class="s3">, </span><span class="s5">0.01</span><span class="s3">, </span><span class="s5">0.1</span><span class="s3">, </span><span class="s5">1</span><span class="s3">],</span>
<span class="s3">}</span>

<span class="s2">param_dist </span><span class="s3">= {</span>
    <span class="s4">'C'</span><span class="s3">: [</span><span class="s5">0.2</span><span class="s3">, </span><span class="s5">4</span><span class="s3">, </span><span class="s5">15</span><span class="s3">, </span><span class="s5">200</span><span class="s3">],</span>
    <span class="s4">'gamma'</span><span class="s3">: [</span><span class="s5">0.1</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">3</span><span class="s3">, </span><span class="s5">7</span><span class="s3">],</span>
<span class="s3">}</span>

<span class="s2">rbf_classifier </span><span class="s3">= </span><span class="s2">SVC</span><span class="s3">(</span><span class="s2">kernel</span><span class="s3">=</span><span class="s4">'rbf'</span><span class="s3">)</span>
<span class="s2">linear_classifier </span><span class="s3">= </span><span class="s2">SVC</span><span class="s3">(</span><span class="s2">kernel</span><span class="s3">=</span><span class="s4">'linear'</span><span class="s3">)</span>

<span class="s2">rbf_grid_search </span><span class="s3">= </span><span class="s2">GridSearchCV</span><span class="s3">(</span><span class="s2">rbf_classifier</span><span class="s3">, </span><span class="s2">param_grid</span><span class="s3">, </span><span class="s2">cv</span><span class="s3">=</span><span class="s5">5</span><span class="s3">)</span>
<span class="s2">rbf_random_search </span><span class="s3">= </span><span class="s2">RandomizedSearchCV</span><span class="s3">(</span><span class="s2">rbf_classifier</span><span class="s3">, </span><span class="s2">param_dist</span><span class="s3">, </span><span class="s2">cv</span><span class="s3">=</span><span class="s5">5</span><span class="s3">, </span><span class="s2">n_iter</span><span class="s3">=</span><span class="s5">10</span><span class="s3">, </span><span class="s2">random_state</span><span class="s3">=</span><span class="s5">39</span><span class="s3">)</span>
<span class="s2">linear_grid_search </span><span class="s3">= </span><span class="s2">GridSearchCV</span><span class="s3">(</span><span class="s2">linear_classifier</span><span class="s3">, </span><span class="s2">param_grid</span><span class="s3">, </span><span class="s2">cv</span><span class="s3">=</span><span class="s5">5</span><span class="s3">)</span>
<span class="s2">linear_random_search </span><span class="s3">= </span><span class="s2">RandomizedSearchCV</span><span class="s3">(</span><span class="s2">linear_classifier</span><span class="s3">, </span><span class="s2">param_dist</span><span class="s3">, </span><span class="s2">cv</span><span class="s3">=</span><span class="s5">5</span><span class="s3">, </span><span class="s2">n_iter</span><span class="s3">=</span><span class="s5">10</span><span class="s3">, </span><span class="s2">random_state</span><span class="s3">=</span><span class="s5">39</span><span class="s3">)</span>


<span class="s2">rbf_grid_search</span><span class="s3">.</span><span class="s2">fit</span><span class="s3">(</span><span class="s2">X_train</span><span class="s3">, </span><span class="s2">y_train</span><span class="s3">)</span>
<span class="s2">rbf_random_search</span><span class="s3">.</span><span class="s2">fit</span><span class="s3">(</span><span class="s2">X_train</span><span class="s3">, </span><span class="s2">y_train</span><span class="s3">)</span>
<span class="s2">linear_grid_search</span><span class="s3">.</span><span class="s2">fit</span><span class="s3">(</span><span class="s2">X_train</span><span class="s3">, </span><span class="s2">y_train</span><span class="s3">)</span>
<span class="s2">linear_random_search</span><span class="s3">.</span><span class="s2">fit</span><span class="s3">(</span><span class="s2">X_train</span><span class="s3">, </span><span class="s2">y_train</span><span class="s3">)</span>

<span class="s2">rbf_accuracy_grid </span><span class="s3">= </span><span class="s2">accuracy_score</span><span class="s3">(</span><span class="s2">y_test</span><span class="s3">, </span><span class="s2">rbf_grid_search</span><span class="s3">.</span><span class="s2">predict</span><span class="s3">(</span><span class="s2">X_test</span><span class="s3">))</span>
<span class="s2">rbf_accuracy_random </span><span class="s3">= </span><span class="s2">accuracy_score</span><span class="s3">(</span><span class="s2">y_test</span><span class="s3">, </span><span class="s2">rbf_random_search</span><span class="s3">.</span><span class="s2">predict</span><span class="s3">(</span><span class="s2">X_test</span><span class="s3">))</span>
<span class="s2">linear_accuracy_grid </span><span class="s3">= </span><span class="s2">accuracy_score</span><span class="s3">(</span><span class="s2">y_test</span><span class="s3">, </span><span class="s2">linear_grid_search</span><span class="s3">.</span><span class="s2">predict</span><span class="s3">(</span><span class="s2">X_test</span><span class="s3">))</span>
<span class="s2">linear_accuracy_random </span><span class="s3">= </span><span class="s2">accuracy_score</span><span class="s3">(</span><span class="s2">y_test</span><span class="s3">, </span><span class="s2">linear_random_search</span><span class="s3">.</span><span class="s2">predict</span><span class="s3">(</span><span class="s2">X_test</span><span class="s3">))</span>

<span class="s2">print</span><span class="s3">(</span><span class="s4">&quot;RBF Kernel - GridSearchCV Accuracy:&quot;</span><span class="s3">, </span><span class="s2">rbf_accuracy_grid</span><span class="s3">)</span>
<span class="s2">print</span><span class="s3">(</span><span class="s4">&quot;RBF Kernel - RandomizedSearchCV Accuracy:&quot;</span><span class="s3">, </span><span class="s2">rbf_accuracy_random</span><span class="s3">)</span>
<span class="s2">print</span><span class="s3">(</span><span class="s4">&quot;Linear Kernel - GridSearchCV Accuracy:&quot;</span><span class="s3">, </span><span class="s2">linear_accuracy_grid</span><span class="s3">)</span>
<span class="s2">print</span><span class="s3">(</span><span class="s4">&quot;Linear Kernel - RandomizedSearchCV Accuracy:&quot;</span><span class="s3">, </span><span class="s2">linear_accuracy_random</span><span class="s3">)</span>
<span class="s0">#%% md 
</span><span class="s2">&lt;b&gt;as out dataset space is not so big we can use GridSearch here&lt;/b&gt; 
</span><span class="s0">#%% md 
</span><span class="s2"># Ploynomial Regression 
</span><span class="s0">#%% md 
</span><span class="s2">To extend the simple linear regression to polynomial regression, we can model the relationship between the independent variable $x$ and the dependent variable $y$ as a polynomial function of degree $n$: 
 
$$f(x) = \beta_0 + \beta_1x + \beta_2x^2 + \ldots + \beta_nx^n$$ 
 
The steps to find the parameters $\beta_i$ are similar to those in simple linear regression. We again minimize the RSS function by taking the derivatives with respect to each parameter and setting them to 0. 
 
- Step 1: Compute the RSS function for polynomial regression: 
 
$$ RSS = \Sigma (y_i - (\hat{\beta_0} + \hat{\beta_1}x_i + \hat{\beta_2}x_i^2 + \ldots + \hat{\beta_n}x_i^n))^2 $$ 
 
- Step 2: Compute the derivatives of the RSS function with respect to each parameter $\beta_i$ and set them to 0 to find the desired parameters. 
 
$$ \frac{\partial RSS}{\partial \beta_i} = 0, \text{ for } i = 0, 1, 2, \ldots, n$$ 
 
Solving these equations will give us the optimal values of $\beta_i$ for the polynomial regression model. The specific form of the equations will depend on the degree of the polynomial and the number of parameters. 
 
The general form for finding the coefficients for polynomial regression can be represented as: 
 
$$ \beta = (X^T X)^{-1} X^T y $$ 
 
where: 
- $X$ is the design matrix with columns $x^0, x^1, x^2, ..., x^n$ 
- $x^i$ represents the feature vector of $x$ raised to the power of $i$ 
- $y$ is the target variable vector 
- $\beta$ is the coefficient vector for the polynomial regression 
 
By solving for $\beta$ using the above formula, we can obtain the coefficients for the polynomial regression model. 
</span><span class="s0">#%% 
</span><span class="s1">def </span><span class="s2">polynomial_regression</span><span class="s3">(</span><span class="s2">x</span><span class="s3">, </span><span class="s2">y</span><span class="s3">, </span><span class="s2">degree</span><span class="s3">):</span>
  <span class="s1">pass</span>
  <span class="s0">#TO DO</span>
<span class="s0">#%% md 
</span><span class="s2">## Computing the Derivative 
 
As we saw, the cost function is the sum over the data points of the squared difference between an observed output and a predicted output. 
 
Since the derivative of a sum is the sum of the derivatives, we can compute the derivative for a single data point and then sum over data points. We can write the squared difference between the observed output and predicted output for a single point as follows: 
 
$$ 
(output  - (const* w _{0} + [feature_1] * w_{1} + ...+ [feature_n] * w_{n}  ))^2 
$$ 
 
With n feautures and a const , So the derivative will be : 
 
 
$$ 
2 * (output  - (const* w _{0} + [feature_1] * w_{1} + ...+ [feature_n] * w_{n}  )) 
$$ 
 
The term inside the paranethesis is just the error (difference between prediction and output). So we can re-write this as: 
 
$$2 * error*[feature_i] $$ 
 
 
That is, the derivative for the weight for feature i is the sum (over data points) of 2 times the product of the error and the feature itself. In the case of the constant then this is just twice the sum of the errors! 
 
Recall that twice the sum of the product of two vectors is just twice the dot product of the two vectors. Therefore the derivative for the weight for feature_i is just two times the dot product between the values of feature_i and the current errors. 
 
 
With this in mind, complete the following derivative function which computes the derivative of the weight given the value of the feature (over all data points) and the errors (over all data points). 
 
</span><span class="s0">#%% 
</span><span class="s1">def </span><span class="s2">feature_derivative</span><span class="s3">(</span><span class="s2">errors</span><span class="s3">, </span><span class="s2">feature</span><span class="s3">):</span>
  <span class="s0">#TO DO</span>
  <span class="s1">pass</span>
<span class="s0">#%% md 
</span><span class="s2">## Gradient Descent 
 
Now we will write a function that performs a gradient descent. The basic premise is simple. Given a starting point we update the current weights by moving in the negative gradient direction. Recall that the gradient is the direction of increase and therefore the negative gradient is the direction of decrease and we're trying to minimize a cost function. 
 
 
The amount by which we move in the negative gradient direction is called the 'step size'. We stop when we are 'sufficiently close' to the optimum. We define this by requiring that the magnitude (length) of the gradient vector to be smaller than a fixed 'tolerance'. 
 
 
With this in mind, complete the following gradient descent function below using your derivative function above. For each step in the gradient descent we update the weight for each feature befofe computing our stopping criteria. 
</span><span class="s0">#%% 
# Utility functions for multiple regression</span>

<span class="s1">def </span><span class="s2">normalize_features</span><span class="s3">(</span><span class="s2">chosen_features</span><span class="s3">, </span><span class="s2">data_frame</span><span class="s3">):</span>
    <span class="s1">for </span><span class="s2">feature </span><span class="s1">in </span><span class="s2">chosen_features</span><span class="s3">:</span>
        <span class="s2">data_frame</span><span class="s3">[</span><span class="s2">feature</span><span class="s3">] = (</span><span class="s2">data_frame</span><span class="s3">[</span><span class="s2">feature</span><span class="s3">] - </span><span class="s2">data_frame</span><span class="s3">[</span><span class="s2">feature</span><span class="s3">].</span><span class="s2">mean</span><span class="s3">()) / </span><span class="s2">data_frame</span><span class="s3">[</span><span class="s2">feature</span><span class="s3">].</span><span class="s2">std</span><span class="s3">()</span>
    <span class="s1">return </span><span class="s2">data_frame</span>

<span class="s1">def </span><span class="s2">predict_output</span><span class="s3">(</span><span class="s2">feature_matrix</span><span class="s3">, </span><span class="s2">weights</span><span class="s3">, </span><span class="s2">bias</span><span class="s3">):</span>
    <span class="s0">#TO DO FOR POLYNOMIAL REGRESSION PREDICTION</span>
    <span class="s1">return </span><span class="s2">predictions</span>


<span class="s0">#%% md 
</span><span class="s2">## Polynomial Regression Using Gradient Descent 
 
Polynomial regression using gradient descent involves finding the optimal parameters for a polynomial model by iteratively updating them based on the gradient of a loss function, typically the Mean Squared Error (MSE). The steps involved are as follows: 
 
- **Step 1: Define the polynomial model** 
The polynomial model has the form: 
$$f(x) = \beta_0 + \beta_1x + \beta_2x^2 + \ldots + \beta_nx^n$$ 
 
- **Step 2: Define the loss function** 
The loss function, such as Mean Squared Error (MSE), measures the error between the actual target values and the predicted values by the model. 
 
- **Step 3: Initialize the coefficients** 
Start with initial guesses for the coefficients $\beta_0, \beta_1, \ldots, \beta_n$ 
 
- **Step 4: Update the coefficients using Gradient Descent** 
Iteratively update the coefficients to minimize the loss function. This is done by computing the gradient of the loss function with respect to each coefficient and making small adjustments in the opposite direction of the gradient. 
 
- **Step 5: Repeat until convergence** 
Continue updating the coefficients iteratively until the algorithm converges to the optimal values. 
 
- **Step 6: Use the learned coefficients for prediction** 
Once the coefficients converge, they can be used in the polynomial function to make predictions on new data points. 
 
Overall, polynomial regression using gradient descent is an iterative optimization process that aims to find the best-fitting polynomial curve to the data points by minimizing the prediction errors. The learning rate and the number of iterations are key hyperparameters to tune for efficient convergence and accurate modeling. 
 
 
</span><span class="s0">#%% 
</span><span class="s1">def </span><span class="s2">polynomial_regression_gradient_descent</span><span class="s3">(</span><span class="s2">feature_matrix</span><span class="s3">, </span><span class="s2">outputs</span><span class="s3">, </span><span class="s2">initial_weights</span><span class="s3">,</span><span class="s2">bias</span><span class="s3">, </span><span class="s2">step_size</span><span class="s3">, </span><span class="s2">tolerance</span><span class="s3">):</span>
    <span class="s2">weights </span><span class="s3">= </span><span class="s2">np</span><span class="s3">.</span><span class="s2">array</span><span class="s3">(</span><span class="s2">initial_weights</span><span class="s3">)</span>

    <span class="s1">while True</span><span class="s3">:</span>
        <span class="s0"># Compute predictions using polynomial function and errors</span>
        <span class="s0">#TO DO</span>

        <span class="s0"># Compute derivatives for all weights</span>
        <span class="s0">#TO DO</span>

        <span class="s0"># Update weights and bias</span>
        <span class="s0">#TO DO</span>

        <span class="s0"># Check convergence</span>
        <span class="s0">#TO DO</span>

    <span class="s1">return </span><span class="s2">weights</span><span class="s3">, </span><span class="s2">bias</span>
<span class="s0">#%% 
</span>
<span class="s1">def </span><span class="s2">run_polynomial_regression</span><span class="s3">(</span><span class="s2">chosen_feature_matrix</span><span class="s3">, </span><span class="s2">target_matrix</span><span class="s3">, </span><span class="s2">keywords</span><span class="s3">):</span>
    <span class="s2">initial_weights </span><span class="s3">= </span><span class="s2">keywords</span><span class="s3">[</span><span class="s4">'initial_weights'</span><span class="s3">]</span>
    <span class="s2">step_size </span><span class="s3">= </span><span class="s2">keywords</span><span class="s3">[</span><span class="s4">'step_size'</span><span class="s3">]</span>
    <span class="s2">tolerance </span><span class="s3">= </span><span class="s2">keywords</span><span class="s3">[</span><span class="s4">'tolerance'</span><span class="s3">]</span>
    <span class="s2">bias </span><span class="s3">= </span><span class="s2">keywords</span><span class="s3">[</span><span class="s4">'bias'</span><span class="s3">]</span>
    <span class="s2">weights </span><span class="s3">= </span><span class="s2">np</span><span class="s3">.</span><span class="s2">array</span><span class="s3">(</span><span class="s2">initial_weights</span><span class="s3">)</span>
    <span class="s2">weights</span><span class="s3">, </span><span class="s2">bias </span><span class="s3">= </span><span class="s2">polynomial_regression_gradient_descent</span><span class="s3">(</span><span class="s2">chosen_feature_matrix</span><span class="s3">, </span><span class="s2">target_matrix</span><span class="s3">, </span><span class="s2">weights</span><span class="s3">, </span><span class="s2">bias</span><span class="s3">, </span><span class="s2">step_size</span><span class="s3">, </span><span class="s2">tolerance</span><span class="s3">)</span>

    <span class="s1">return </span><span class="s2">weights</span><span class="s3">, </span><span class="s2">bias</span>

<span class="s1">def </span><span class="s2">get_weights_and_bias</span><span class="s3">(</span><span class="s2">chosen_features</span><span class="s3">):</span>

    <span class="s2">keywords </span><span class="s3">= {</span>
        <span class="s4">'initial_weights'</span><span class="s3">: </span><span class="s2">np</span><span class="s3">.</span><span class="s2">array</span><span class="s3">([</span><span class="s5">.5</span><span class="s3">]*</span><span class="s2">len</span><span class="s3">(</span><span class="s2">chosen_features</span><span class="s3">)),</span>
        <span class="s4">'step_size'</span><span class="s3">: </span><span class="s5">1.e-4</span><span class="s3">,</span>
        <span class="s4">'tolerance'</span><span class="s3">: </span><span class="s5">1.e-10</span><span class="s3">,</span>
        <span class="s4">'bias'</span><span class="s3">: </span><span class="s5">0</span>
    <span class="s3">}</span>

    <span class="s0"># TO DO</span>

    <span class="s1">return </span><span class="s2">chosen_feature_matrix</span><span class="s3">, </span><span class="s2">train_weights</span><span class="s3">, </span><span class="s2">bias</span></pre>
</body>
</html>